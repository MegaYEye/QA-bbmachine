{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents.json\") as f:\n",
    "    documents = json.load(f)\n",
    "    # max len 653 min len 20\n",
    "with open(\"training.json\") as f:\n",
    "    training_document = json.load(f)\n",
    "with open(\"devel.json\") as f:\n",
    "    dev_document = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = [None] * 441\n",
    "for d in documents:\n",
    "    document_list[d[\"docid\"]]= d[\"text\"]\n",
    "    \n",
    "    \n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.hist(m, bins=100, color='steelblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_questions = [] \\nres = 0\\nlres = []\\nfor q in training:\\n    para = document_list[q[\"docid\"]][q[\"answer_paragraph\"]].lower().split(\".\")\\n    count = 0\\n    ans = q[\"text\"].lower()\\n    for sent in para:\\n        if ans in sent:#.split(\" \"):\\n            count += 1\\n    if count>1:\\n        res +=1\\n        lres.append({\"q\":q,\"p\":para})\\n    elif count == 1:\\n        training_questions.append({\"q\":q,\"p\":para})\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_questions = [] \n",
    "res = 0\n",
    "lres = []\n",
    "for q in training:\n",
    "    para = document_list[q[\"docid\"]][q[\"answer_paragraph\"]].lower().split(\".\")\n",
    "    count = 0\n",
    "    ans = q[\"text\"].lower()\n",
    "    for sent in para:\n",
    "        if ans in sent:#.split(\" \"):\n",
    "            count += 1\n",
    "    if count>1:\n",
    "        res +=1\n",
    "        lres.append({\"q\":q,\"p\":para})\n",
    "    elif count == 1:\n",
    "        training_questions.append({\"q\":q,\"p\":para})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "\n",
    "char_numbers = {}\n",
    "char_names = []\n",
    "\n",
    "for d in document_list:\n",
    "    for para in d:\n",
    "        for char in para:\n",
    "            char_numbers[char] = char_numbers.setdefault(char, len(char_numbers))\n",
    "            \n",
    "for q in training_document:\n",
    "    for char in q[\"question\"]:\n",
    "        char_numbers[char] = char_numbers.setdefault(char, len(char_numbers))\n",
    "    \n",
    "char_numbers[\"<a>\"] = char_numbers.setdefault(\"<a>\", len(char_numbers))\n",
    "char_numbers[\"<unk>\"] = char_numbers.setdefault(\"<unk>\", len(char_numbers))\n",
    "\n",
    "char_names = [None] * len(char_numbers)\n",
    "for char, index in char_numbers.items():\n",
    "    char_names[index] = char\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split with length and pad\n",
    "def str_padding(string,length):\n",
    "    words = string.split(\" \")\n",
    "    res = []\n",
    "    temp = \"\"\n",
    "    for word in words:\n",
    "        if len(temp + \" \" + word) > length:\n",
    "            t = [\"<a>\"]*length\n",
    "            t[0:len(temp)] = list(temp)\n",
    "            res.append(t)\n",
    "            temp = \"\"\n",
    "        if (len(temp)):\n",
    "            temp = temp + \" \" + word\n",
    "        else:\n",
    "            temp = word\n",
    "    t = [\"<a>\"]*length\n",
    "    t[0:len(temp)] = list(temp)\n",
    "    res.append(t)\n",
    "    return res\n",
    "\n",
    "def sent2vec(sent):\n",
    "    res = []\n",
    "    for n in sent:\n",
    "        res.append(char_numbers[n])\n",
    "    return res\n",
    "\n",
    "def vec2sent(arr):\n",
    "    res = []\n",
    "    for n in arr:\n",
    "        if not char_names[n] == \"<a>\":\n",
    "            res.append(char_names[n])\n",
    "    res = \"\".join(res)\n",
    "    res.replace(\"<a>\",\"\")\n",
    "    return res\n",
    "\n",
    "def set_divide(dataset,length):\n",
    "    training_set = []\n",
    "    set_count = 0\n",
    "    poc = 0\n",
    "    nec = 0\n",
    "    for question in dataset:\n",
    "        para = document_list[question[\"docid\"]][question[\"answer_paragraph\"]]\n",
    "        que = question[\"question\"]\n",
    "        answer = question[\"text\"]\n",
    "        if para.lower().count(answer.lower(),0,len(para)) == 1:\n",
    "            po_set = []\n",
    "            ne_set = []\n",
    "            sents =  str_padding(para,seq_len)\n",
    "            for sent in sents:\n",
    "                tsent = \"\".join(sent).lower()\n",
    "                if answer.lower() in tsent:\n",
    "                    po_set.append(sent2vec(sent))\n",
    "                else:\n",
    "                    ne_set.append(sent2vec(sent))\n",
    "            if len(po_set) == 1:\n",
    "                t = {}\n",
    "                t[\"question\"] = [sent2vec(arr) for arr in str_padding(que,length)]\n",
    "                t[\"set\"] = [po_set,ne_set]\n",
    "                t[\"answer\"] = [sent2vec(arr) for arr in str_padding(answer,length)]\n",
    "                training_set.append(t)\n",
    "                set_count += 1\n",
    "                poc += len(po_set)\n",
    "                nec += len(ne_set)\n",
    "    print (\"po count:\",poc,\" ne count:\",nec,\"dataset used:\",set_count)\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_generate(dataset):\n",
    "    np.random.shuffle(dataset)\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in dataset:\n",
    "        for s in d[\"set\"][0]:\n",
    "            t = np.append(s,d[\"question\"])\n",
    "            X.append(t)\n",
    "            y.append(1)\n",
    "        for s in d[\"set\"][1]:\n",
    "            t = np.append(s,d[\"question\"])\n",
    "            X.append(t)\n",
    "            y.append(0)\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\envs\\PT\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape,hidden_unit,filter_num):\n",
    "    inp = Input(shape = (input_shape,))\n",
    "    embedding = Embedding(input_dim = len(char_names),output_dim = seq_len)(inp)\n",
    "    conv1= Convolution1D(hidden_unit, filter_num, activation='relu')(embedding)\n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    dense1 = Dense(hidden_unit)(pool1)\n",
    "    dense2 = Dense(hidden_unit)(dense1)\n",
    "    res = Dense(1,activation = \"sigmoid\")(dense2)\n",
    "\n",
    "    model = Model(input=[inp], output=[res])\n",
    "    model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po count: 33858  ne count: 35715 dataset used: 33858\n",
      "po count: 2415  ne count: 2628 dataset used: 2415\n",
      "(69573, 1000) (69573,)\n",
      "(4043, 1000) (4043,)\n",
      "(1000, 1000) (1000,)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 500\n",
    "training_set = set_divide(training_document,seq_len)\n",
    "train_X,train_y = set_generate(training_set)\n",
    "\n",
    "dev_set = set_divide(dev_document,seq_len)\n",
    "dev_X,dev_y = set_generate(dev_set)\n",
    "\n",
    "test_X = dev_X[0:1000]\n",
    "test_y = dev_y[0:1000]\n",
    "dev_X = dev_X[1000:]\n",
    "dev_y = dev_y[1000:]\n",
    "\n",
    "print (train_X.shape,train_y.shape)\n",
    "print (dev_X.shape,dev_y.shape)\n",
    "print (test_X.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_unit:  50  filter:  9\n",
      "WARNING:tensorflow:From L:\\Anaconda3\\envs\\PT\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\envs\\PT\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69573 samples, validate on 4043 samples\n",
      "Epoch 1/1\n",
      "69573/69573 [==============================] - 77s 1ms/step - loss: 0.2079 - acc: 0.6768 - val_loss: 0.2123 - val_acc: 0.6691\n",
      "1000/1000 [==============================] - 0s 332us/step\n",
      "(50, 9) [0.20715678918361663, 0.68]\n"
     ]
    }
   ],
   "source": [
    "eva = {}\n",
    "#hid = [20,50]\n",
    "#fil = [3,5,9,19]\n",
    "hid = [50]\n",
    "fil = [9]\n",
    "\n",
    "for h in hid:\n",
    "    for f in fil:\n",
    "        print (\"hidden_unit: \",h,\" filter: \",f)\n",
    "        model = get_model(seq_len*2,h,f)\n",
    "        model.fit(train_X,train_y,epochs = 1,validation_data = (dev_X,dev_y))\n",
    "        eva[(h,f)] = model.evaluate(test_X,test_y)\n",
    "        print (str((h,f)),eva[(h,f)])\n",
    "        model.save(\"models/unit_\"+str(h)+\"_filter_\"+str(f)+\"_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(50, 9): [0.20715678918361663, 0.68]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1=model\n",
    "eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total = 0\n",
    "pos = 0\n",
    "for d in dev_set:\n",
    "    if len(d[\"set\"][1])>0:\n",
    "        po = np.array([np.append(t,d[\"question\"]) for t in d[\"set\"][0]])\n",
    "        ne = np.array([np.append(t,d[\"question\"]) for t in d[\"set\"][1]])\n",
    "        if model.predict(po)[0] > np.max(model.predict(ne)):\n",
    "            pos+=1\n",
    "        total +=1\n",
    "print (pos/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eva(pred,true):\n",
    "    pred = [s.lower() for s in pred]\n",
    "    true = [s.lower() for s in true]\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    r = 0\n",
    "    for i in pred:\n",
    "        if i in true:\n",
    "            tp+=1\n",
    "    fp = len(pred) - tp\n",
    "    for i in true:\n",
    "        if i not in pred:\n",
    "            fn += 1\n",
    "    return 2*tp / (2*tp + fn+fp+0.000001)\n",
    "\n",
    "def n_gram_cut(string,n):\n",
    "    res = []\n",
    "    if \" \" not in string:\n",
    "        return [string]\n",
    "    string = string.split(\" \")\n",
    "    slen = len(string)\n",
    "    for i in range(1,min(n,slen),1):\n",
    "        temp = [\" \".join(string[j:j+i+1]) for j in range(slen-i)]\n",
    "        res = res + temp\n",
    "    return res\n",
    "\n",
    "def answer_train_generate(dataset,length,n_gram,prob = 0.2):\n",
    "    set_X = []\n",
    "    set_y = []\n",
    "    count = 0\n",
    "    total = len(dataset)\n",
    "    setc = 0\n",
    "    for d in dataset:\n",
    "        answer = vec2sent(d[\"answer\"][0]).split(' ')\n",
    "        que1 = d[\"question\"][0]\n",
    "        doc2 = d[\"set\"][0][0]\n",
    "        cors = n_gram_cut(vec2sent(d[\"set\"][0][0]),n_gram)\n",
    "        for cor in cors:\n",
    "            f1 = f1_eva(cor.split(\" \"),answer)\n",
    "            if f1>0 and np.random.uniform()<prob:\n",
    "                cor3 = sent2vec(str_padding(cor,length)[0])\n",
    "                set_X.append(que1+doc2+cor3)\n",
    "                set_y.append(f1**2)\n",
    "                setc+=1\n",
    "        count +=1\n",
    "        if count % 100 == 0:\n",
    "            print (\"{}/{}, total count:{}\".format(count,total,setc),end=\"\\r\")\n",
    "    print (\"{}/{}, total count:{}\".format(count,total,setc))\n",
    "    return np.array(set_X),np.array(set_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33858/33858, total count:277659\n",
      "2415/2415, total count:18243\n"
     ]
    }
   ],
   "source": [
    "train_cor_X,train_cor_y = answer_train_generate(training_set,seq_len,4,prob = 0.5)\n",
    "dev_cor_X,dev_cor_y = answer_train_generate(dev_set,seq_len,4,prob = 0.5)\n",
    "\n",
    "test_cor_X = dev_cor_X[0:2000]\n",
    "test_cor_y = dev_cor_y[0:2000]\n",
    "dev_cor_X = dev_cor_X[2000:]\n",
    "dev_cor_y = dev_cor_y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  67,   26,    8, ..., 1372, 1372, 1372])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_cor_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_unit:  100  filter:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\envs\\PT\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 277659 samples, validate on 16243 samples\n",
      "Epoch 1/1\n",
      "277659/277659 [==============================] - 1222s 4ms/step - loss: 0.0354 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00\n",
      "2000/2000 [==============================] - 3s 2ms/step\n",
      "(100, 19) [0.04000285120308399, 0.0]\n"
     ]
    }
   ],
   "source": [
    "eva = {}\n",
    "#hid = [20,50]\n",
    "#fil = [3,5,9,19]\n",
    "hid = [100]\n",
    "fil = [19]\n",
    "\n",
    "for h in hid:\n",
    "    for f in fil:\n",
    "        print (\"hidden_unit: \",h,\" filter: \",f)\n",
    "        model = get_model(seq_len*3,h,f)\n",
    "        model.fit(train_cor_X,train_cor_y,epochs = 1,validation_data = (dev_cor_X,dev_cor_y))\n",
    "        eva[(h,f)] = model.evaluate(test_cor_X,test_cor_y)\n",
    "        print (str((h,f)),eva[(h,f)])\n",
    "        model.save(\"models/unit_\"+str(h)+\"_filter_\"+str(f)+\"_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2410/2415\r"
     ]
    }
   ],
   "source": [
    "total = len(dev_set)\n",
    "count = 0\n",
    "f = []\n",
    "for d in dev_set:\n",
    "    #d = dev_set[0]\n",
    "    X = []\n",
    "    que1 = d[\"question\"][0]\n",
    "    doc2 = d[\"set\"][0][0]\n",
    "    para = vec2sent(doc2)\n",
    "    cors = n_gram_cut(para,4)\n",
    "    for cor in cors:\n",
    "        cor3 = sent2vec(str_padding(cor,seq_len)[0])\n",
    "        X.append(que1+doc2+cor3)\n",
    "    X = np.array(X)\n",
    "    res = model2.predict(X)\n",
    "    index= np.argmax(res)\n",
    "    ans = vec2sent(X[index][1000:1500])\n",
    "    f.append(f1_eva(ans,vec2sent(d[\"answer\"][0])))\n",
    "    count +=1\n",
    "    if count %10 == 0:\n",
    "        print (\"{}/{}\".format(count,total),end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who won the war?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2sent(X[5][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After the Russian Revolution of 1917 guerrillas throughout Central Asia, known as basmachi, waged a war against Bolshevik armies in a futile attempt to maintain independence. The Bolsheviks prevailed after a four-year war, in which mosques and villages were burned down and the population heavily suppressed. Soviet authorities started a campaign of secularization, practicing Islam, Judaism, and Christianity was discouraged and repressed, and many mosques, churches, and synagogues were closed. As'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2sent(X[5][500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1917'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2sent(X[5][1000:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the bolsheviks'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2sent(d[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35672325061795745"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4347825897920613,\n",
       " 0.689655148632581,\n",
       " 0.2857142653061239,\n",
       " 0.9999999545454565,\n",
       " 0.9473683711911383,\n",
       " 0.7272726942148775,\n",
       " 0.999999958333335,\n",
       " 0.5714285551020413,\n",
       " 0.0,\n",
       " 0.6999999650000017,\n",
       " 0.11111110493827195,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.29999998500000075,\n",
       " 0.0,\n",
       " 0.399999980000001,\n",
       " 0.639999974400001,\n",
       " 0.4210526094182837,\n",
       " 0.5294117491349486,\n",
       " 0.2608695538752368,\n",
       " 0.3846153698224858,\n",
       " 0.0,\n",
       " 0.41379308917954866,\n",
       " 0.9999999166666736,\n",
       " 0.08695651795841226,\n",
       " 0.19047618140589612,\n",
       " 0.620689633769323,\n",
       " 0.4210526094182837,\n",
       " 0.5333332977777802,\n",
       " 0.21052630470914185,\n",
       " 0.3846153698224858,\n",
       " 0.0,\n",
       " 0.588235259515573,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42857139795918586,\n",
       " 0.0,\n",
       " 0.6086956257088858,\n",
       " 0.4444444197530878,\n",
       " 0.0,\n",
       " 0.17647058304498286,\n",
       " 0.4999999375000079,\n",
       " 0.36363635261708027,\n",
       " 0.0,\n",
       " 0.36363634710743875,\n",
       " 0.13333332444444504,\n",
       " 0.4545454338842984,\n",
       " 0.0,\n",
       " 0.5714285510204089,\n",
       " 0.0,\n",
       " 0.11111110493827195,\n",
       " 0.3333333055555579,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4999999750000012,\n",
       " 0.38095236281179223,\n",
       " 0.29999998500000075,\n",
       " 0.6249999804687506,\n",
       " 0.0,\n",
       " 0.16666665277777895,\n",
       " 0.23529410380622917,\n",
       " 0.12499999218750048,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.599999940000006,\n",
       " 0.7741935234131121,\n",
       " 0.23529410380622917,\n",
       " 0.4347825897920613,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4545454338842984,\n",
       " 0.7999999771428579,\n",
       " 0.8108107888970058,\n",
       " 0.3199999872000005,\n",
       " 0.5294117491349486,\n",
       " 0.9999999545454565,\n",
       " 0.28571427210884415,\n",
       " 0.4999999750000012,\n",
       " 0.39999997333333515,\n",
       " 0.1999999900000005,\n",
       " 0.0,\n",
       " 0.34782607183364905,\n",
       " 0.99999990000001,\n",
       " 0.0,\n",
       " 0.49999998214285774,\n",
       " 0.34782607183364905,\n",
       " 0.0,\n",
       " 0.49999998214285774,\n",
       " 0.36363634710743875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.37499997656250145,\n",
       " 0.4999999583333368,\n",
       " 0.09090908677685969,\n",
       " 0.13333332444444504,\n",
       " 0.23999999040000036,\n",
       " 0.615384568047341,\n",
       " 0.5185184993141296,\n",
       " 0.5185184993141296,\n",
       " 0.5555555401234572,\n",
       " 0.0,\n",
       " 0.22222219753086697,\n",
       " 0.0,\n",
       " 0.874999972656251,\n",
       " 0.34782607183364905,\n",
       " 0.7058823114186875,\n",
       " 0.39999998857142893,\n",
       " 0.0,\n",
       " 0.07999999680000013,\n",
       " 0.333333319444445,\n",
       " 0.0,\n",
       " 0.4347825897920613,\n",
       " 0.2222222098765439,\n",
       " 0.23076922189349144,\n",
       " 0.0,\n",
       " 0.07999999680000013,\n",
       " 0.13793102972651622,\n",
       " 0.3636363305785154,\n",
       " 0.4651162682531101,\n",
       " 0.6666665925926009,\n",
       " 0.8999999550000022,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.28571427210884415,\n",
       " 0.5333333155555561,\n",
       " 0.9999999444444475,\n",
       " 0.7272726611570308,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.5714285442176883,\n",
       " 0.3333333148148158,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.28571427210884415,\n",
       " 0.16666665277777895,\n",
       " 0.0,\n",
       " 0.9999999444444475,\n",
       " 0.0,\n",
       " 0.39999998400000064,\n",
       " 0.0,\n",
       " 0.5555555246913597,\n",
       " 0.6999999650000017,\n",
       " 0.5517241189060649,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666665925926009,\n",
       " 0.9999998333333611,\n",
       " 0.45161288865764876,\n",
       " 0.9999999545454565,\n",
       " 0.0,\n",
       " 0.23076922189349144,\n",
       " 0.3199999872000005,\n",
       " 0.5714285510204089,\n",
       " 0.23529410380622917,\n",
       " 0.44444442798353967,\n",
       " 0.99999990000001,\n",
       " 0.0,\n",
       " 0.6923076656804744,\n",
       " 0.0,\n",
       " 0.41666664930555625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3199999872000005,\n",
       " 0.7857142576530622,\n",
       " 0.15384614201183525,\n",
       " 0.44444439506173394,\n",
       " 0.3199999872000005,\n",
       " 0.4210526094182837,\n",
       " 0.5454545206611581,\n",
       " 0.799999960000002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.47058820761245834,\n",
       " 0.5217391077504736,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1999999900000005,\n",
       " 0.9999998750000157,\n",
       " 0.0,\n",
       " 0.9999998333333611,\n",
       " 0.8108107888970058,\n",
       " 0.49999998437500054,\n",
       " 0.3333333148148158,\n",
       " 0.0,\n",
       " 0.1999999900000005,\n",
       " 0.8333332986111125,\n",
       " 0.5599999776000009,\n",
       " 0.0,\n",
       " 0.8666666377777787,\n",
       " 0.4799999808000007,\n",
       " 0.28571427551020445,\n",
       " 0.36363634710743875,\n",
       " 0.23529410380622917,\n",
       " 0.6666665925926009,\n",
       " 0.999999958333335,\n",
       " 0.9999999500000024,\n",
       " 0.5185184993141296,\n",
       " 0.8799999648000014,\n",
       " 0.0,\n",
       " 0.14285713265306196,\n",
       " 0.47619045351474026,\n",
       " 0.34782607183364905,\n",
       " 0.5555555246913597,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39999996000000404,\n",
       " 0.4210526094182837,\n",
       " 0.9999998750000157,\n",
       " 0.7499999062500118,\n",
       " 0.0,\n",
       " 0.41666664930555625,\n",
       " 0.0,\n",
       " 0.639999974400001,\n",
       " 0.7272727052341605,\n",
       " 0.0,\n",
       " 0.4999999791666675,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666296296316,\n",
       " 0.7222222021604945,\n",
       " 0.4347825897920613,\n",
       " 0.7999999771428579,\n",
       " 0.22222219753086697,\n",
       " 0.6666666111111158,\n",
       " 0.0,\n",
       " 0.5333332977777802,\n",
       " 0.15384614201183525,\n",
       " 0.11764705190311459,\n",
       " 0.6956521436672981,\n",
       " 0.10526315512465383,\n",
       " 0.9999999444444475,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7999999466666703,\n",
       " 0.0,\n",
       " 0.2962962853223598,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4545454338842984,\n",
       " 0.6470588044982705,\n",
       " 0.4799999808000007,\n",
       " 0.484848470156107,\n",
       " 0.0,\n",
       " 0.6046511487290431,\n",
       " 0.22222219753086697,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3636363305785154,\n",
       " 0.48275860404280674,\n",
       " 0.0,\n",
       " 0.2962962853223598,\n",
       " 0.49999998214285774,\n",
       " 0.9999998750000157,\n",
       " 0.5925925706447196,\n",
       " 0.8333332986111125,\n",
       " 0.399999980000001,\n",
       " 0.11764705190311459,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39999998400000064,\n",
       " 0.6086956257088858,\n",
       " 0.22222219753086697,\n",
       " 0.11764705190311459,\n",
       " 0.4999999375000079,\n",
       " 0.5833333090277788,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5185184993141296,\n",
       " 0.9999999166666736,\n",
       " 0.15384614201183525,\n",
       " 0.27272726033057904,\n",
       " 0.8461538136094686,\n",
       " 0.0,\n",
       " 0.9999999166666736,\n",
       " 0.4545454338842984,\n",
       " 0.4444444197530878,\n",
       " 0.5806451425598341,\n",
       " 0.09999999500000024,\n",
       " 0.2857142653061239,\n",
       " 0.0,\n",
       " 0.5333332977777802,\n",
       " 0.5185184993141296,\n",
       " 0.16666665277777895,\n",
       " 0.639999974400001,\n",
       " 0.999999958333335,\n",
       " 0.5517241189060649,\n",
       " 0.8749999453125034,\n",
       " 0.13333332444444504,\n",
       " 0.2666666488888901,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5999999700000015,\n",
       " 0.5555555246913597,\n",
       " 0.4615384437869829,\n",
       " 0.4545454338842984,\n",
       " 0.639999974400001,\n",
       " 0.3333333148148158,\n",
       " 0.23529410380622917,\n",
       " 0.4347825897920613,\n",
       " 0.5161290156087414,\n",
       " 0.0,\n",
       " 0.2222222098765439,\n",
       " 0.5384615177514801,\n",
       " 0.9999999615384629,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7368420664819965,\n",
       " 0.66666663888889,\n",
       " 0.5263157617728547,\n",
       " 0.0,\n",
       " 0.6666666444444451,\n",
       " 0.49999998214285774,\n",
       " 0.7741935234131121,\n",
       " 0.5517241189060649,\n",
       " 0.9411764152249167,\n",
       " 0.5454545206611581,\n",
       " 0.6666666111111158,\n",
       " 0.13333332444444504,\n",
       " 0.38095236281179223,\n",
       " 0.3199999872000005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9999999500000024,\n",
       " 0.0,\n",
       " 0.4999999687500019,\n",
       " 0.0,\n",
       " 0.4347825897920613,\n",
       " 0.0,\n",
       " 0.5294117491349486,\n",
       " 0.4999999375000079,\n",
       " 0.6666665925926009,\n",
       " 0.36363634710743875,\n",
       " 0.639999974400001,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.49999998214285774,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.2666666488888901,\n",
       " 0.64285711989796,\n",
       " 0.399999980000001,\n",
       " 0.35294115570934376,\n",
       " 0.31578945706371275,\n",
       " 0.0,\n",
       " 0.2857142653061239,\n",
       " 0.4545454338842984,\n",
       " 0.17391303591682453,\n",
       " 0.9999998750000157,\n",
       " 0.5714285442176883,\n",
       " 0.5185184993141296,\n",
       " 0.0,\n",
       " 0.5454545206611581,\n",
       " 0.2222222098765439,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2222222098765439,\n",
       " 0.7619047256235845,\n",
       " 0.399999980000001,\n",
       " 0.14285713265306196,\n",
       " 0.5599999776000009,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666419753096,\n",
       " 0.4347825897920613,\n",
       " 0.6956521436672981,\n",
       " 0.6666666349206364,\n",
       " 0.4210526094182837,\n",
       " 0.4444444197530878,\n",
       " 0.6666665925926009,\n",
       " 0.15999999360000025,\n",
       " 0.29999998500000075,\n",
       " 0.44444439506173394,\n",
       " 0.3333333148148158,\n",
       " 0.9999999545454565,\n",
       " 0.3199999872000005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09523809070294806,\n",
       " 0.5454545289256204,\n",
       " 0.9999998333333611,\n",
       " 0.2222222098765439,\n",
       " 0.47058820761245834,\n",
       " 0.5217391077504736,\n",
       " 0.0,\n",
       " 0.24999996875000394,\n",
       " 0.0,\n",
       " 0.5517241189060649,\n",
       " 0.4444444197530878,\n",
       " 0.9999999642857155,\n",
       " 0.4666666511111116,\n",
       " 0.14285713265306196,\n",
       " 0.2222222098765439,\n",
       " 0.4615384260355057,\n",
       " 0.2222222098765439,\n",
       " 0.7407407133058995,\n",
       " 0.0,\n",
       " 0.7999999733333342,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5999999700000015,\n",
       " 0.41379308917954866,\n",
       " 0.0,\n",
       " 0.09090908677685969,\n",
       " 0.6666665925926009,\n",
       " 0.31578945706371275,\n",
       " 0.47058820761245834,\n",
       " 0.4210526094182837,\n",
       " 0.689655148632581,\n",
       " 0.0,\n",
       " 0.4444444197530878,\n",
       " 0.0,\n",
       " 0.1818181652892577,\n",
       " 0.2608695538752368,\n",
       " 0.0,\n",
       " 0.37499997656250145,\n",
       " 0.0,\n",
       " 0.5185184993141296,\n",
       " 0.999999958333335,\n",
       " 0.34782607183364905,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15384614201183525,\n",
       " 0.22222219753086697,\n",
       " 0.4999999791666675,\n",
       " 0.19047618140589612,\n",
       " 0.45161288865764876,\n",
       " 0.19999998000000202,\n",
       " 0.4545454338842984,\n",
       " 0.5217391077504736,\n",
       " 0.7999999771428579,\n",
       " 0.9999999444444475,\n",
       " 0.2608695538752368,\n",
       " 0.6666666419753096,\n",
       " 0.0,\n",
       " 0.6086956257088858,\n",
       " 0.5806451425598341,\n",
       " 0.9999999500000024,\n",
       " 0.0,\n",
       " 0.615384568047341,\n",
       " 0.639999974400001,\n",
       " 0.0,\n",
       " 0.27272726033057904,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7692307396449716,\n",
       " 0.5333333155555561,\n",
       " 0.28571427210884415,\n",
       " 0.6086956257088858,\n",
       " 0.0,\n",
       " 0.10526315235457093,\n",
       " 0.0,\n",
       " 0.47058820761245834,\n",
       " 0.5263157617728547,\n",
       " 0.4999999791666675,\n",
       " 0.0,\n",
       " 0.23529410380622917,\n",
       " 0.0,\n",
       " 0.615384568047341,\n",
       " 0.66666663888889,\n",
       " 0.23999999040000036,\n",
       " 0.4545454338842984,\n",
       " 0.9999999615384629,\n",
       " 0.9090908677685968,\n",
       " 0.4999999750000012,\n",
       " 0.4615384437869829,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38095236281179223,\n",
       " 0.31578945706371275,\n",
       " 0.29999998500000075,\n",
       " 0.0,\n",
       " 0.7368420664819965,\n",
       " 0.0,\n",
       " 0.3076922840236705,\n",
       " 0.12499999218750048,\n",
       " 0.4347825897920613,\n",
       " 0.9285713954081645,\n",
       " 0.5384615177514801,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15384614201183525,\n",
       " 0.0,\n",
       " 0.19047618140589612,\n",
       " 0.4444444197530878,\n",
       " 0.28571427210884415,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7692307396449716,\n",
       " 0.333333319444445,\n",
       " 0.4999999687500019,\n",
       " 0.639999974400001,\n",
       " 0.4999999791666675,\n",
       " 0.31578945706371275,\n",
       " 0.0,\n",
       " 0.44444442798353967,\n",
       " 0.5185184993141296,\n",
       " 0.7142856887755111,\n",
       " 0.12499999218750048,\n",
       " 0.39999997333333515,\n",
       " 0.5217391077504736,\n",
       " 0.5599999776000009,\n",
       " 0.7142856887755111,\n",
       " 0.6956521436672981,\n",
       " 0.5333332977777802,\n",
       " 0.4615384437869829,\n",
       " 0.0,\n",
       " 0.6999999650000017,\n",
       " 0.1999999900000005,\n",
       " 0.484848470156107,\n",
       " 0.0,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.13333332444444504,\n",
       " 0.07999999680000013,\n",
       " 0.39999996000000404,\n",
       " 0.6315789141274255,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.5454545206611581,\n",
       " 0.4545454338842984,\n",
       " 0.2962962853223598,\n",
       " 0.4999999375000079,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.11111110493827195,\n",
       " 0.2222222098765439,\n",
       " 0.4999999583333368,\n",
       " 0.6666666444444451,\n",
       " 0.47058820761245834,\n",
       " 0.38095236281179223,\n",
       " 0.8571428163265326,\n",
       " 0.48275860404280674,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4999999375000079,\n",
       " 0.0,\n",
       " 0.9999999705882362,\n",
       " 0.9999999500000024,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11111110493827195,\n",
       " 0.31578945706371275,\n",
       " 0.5833333090277788,\n",
       " 0.0,\n",
       " 0.12499999218750048,\n",
       " 0.0,\n",
       " 0.47619045351474026,\n",
       " 0.4347825897920613,\n",
       " 0.28571427210884415,\n",
       " 0.0,\n",
       " 0.9999999285714337,\n",
       " 0.5555555246913597,\n",
       " 0.6666665925926009,\n",
       " 0.8333332986111125,\n",
       " 0.6153845917159773,\n",
       " 0.3846153698224858,\n",
       " 0.0,\n",
       " 0.36363634710743875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5925925706447196,\n",
       " 0.8799999648000014,\n",
       " 0.0,\n",
       " 0.19047618140589612,\n",
       " 0.27586205945303244,\n",
       " 0.0,\n",
       " 0.999999958333335,\n",
       " 0.0,\n",
       " 0.6060605876951337,\n",
       " 0.0,\n",
       " 0.5333333155555561,\n",
       " 0.0,\n",
       " 0.6923076656804744,\n",
       " 0.0,\n",
       " 0.4666666511111116,\n",
       " 0.47619045351474026,\n",
       " 0.7272726611570308,\n",
       " 0.8124999746093758,\n",
       " 0.4615384260355057,\n",
       " 0.26666665777777804,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.399999980000001,\n",
       " 0.13333332444444504,\n",
       " 0.4545454338842984,\n",
       " 0.6666666296296316,\n",
       " 0.0,\n",
       " 0.35714284438775556,\n",
       " 0.7499999062500118,\n",
       " 0.8333333101851859,\n",
       " 0.6086956257088858,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5217391077504736,\n",
       " 0.0,\n",
       " 0.5454544958677732,\n",
       " 0.4799999808000007,\n",
       " 0.0,\n",
       " 0.37499997656250145,\n",
       " 0.2666666488888901,\n",
       " 0.0,\n",
       " 0.18181817355371938,\n",
       " 0.7619047256235845,\n",
       " 0.31578945706371275,\n",
       " 0.4210526094182837,\n",
       " 0.5833333090277788,\n",
       " 0.4545454338842984,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3076922840236705,\n",
       " 0.47058820761245834,\n",
       " 0.5599999776000009,\n",
       " 0.4285714132653067,\n",
       " 0.2857142653061239,\n",
       " 0.5999999700000015,\n",
       " 0.42424241138659363,\n",
       " 0.6315789307479229,\n",
       " 0.31578945706371275,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.35294115570934376,\n",
       " 0.0,\n",
       " 0.5925925706447196,\n",
       " 0.7586206634958391,\n",
       " 0.14285713265306196,\n",
       " 0.0,\n",
       " 0.1999999900000005,\n",
       " 0.5555555246913597,\n",
       " 0.6666666464646471,\n",
       " 0.0,\n",
       " 0.19354838085327802,\n",
       " 0.4799999808000007,\n",
       " 0.2857142653061239,\n",
       " 0.13793102972651622,\n",
       " 0.11111110493827195,\n",
       " 0.0,\n",
       " 0.08333332986111125,\n",
       " 0.0,\n",
       " 0.27272726033057904,\n",
       " 0.38095236281179223,\n",
       " 0.4999999791666675,\n",
       " 0.6666666296296316,\n",
       " 0.0,\n",
       " 0.28571427551020445,\n",
       " 0.4799999808000007,\n",
       " 0.44444439506173394,\n",
       " 0.399999980000001,\n",
       " 0.9999998333333611,\n",
       " 0.0,\n",
       " 0.7999999733333342,\n",
       " 0.0,\n",
       " 0.21052630470914185,\n",
       " 0.6999999650000017,\n",
       " 0.41379308917954866,\n",
       " 0.0,\n",
       " 0.6956521436672981,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4666666511111116,\n",
       " 0.23999999040000036,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1999999900000005,\n",
       " 0.39999996000000404,\n",
       " 0.21052630470914185,\n",
       " 0.9999999666666678,\n",
       " 0.7692307396449716,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7999999680000013,\n",
       " 0.57142848979593,\n",
       " 0.47619045351474026,\n",
       " 0.0,\n",
       " 0.3199999872000005,\n",
       " 0.5599999776000009,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.639999974400001,\n",
       " 0.620689633769323,\n",
       " 0.05555555401234573,\n",
       " 0.6956521436672981,\n",
       " 0.3076922840236705,\n",
       " 0.41666664930555625,\n",
       " 0.0,\n",
       " 0.31578945706371275,\n",
       " 0.7826086616257103,\n",
       " 0.5142856995918372,\n",
       " 0.7199999712000011,\n",
       " 0.28571427210884415,\n",
       " 0.4210526094182837,\n",
       " 0.4210526094182837,\n",
       " 0.6666666296296316,\n",
       " 0.7142856632653098,\n",
       " 0.0,\n",
       " 0.6060605876951337,\n",
       " 0.0,\n",
       " 0.41379308917954866,\n",
       " 0.42424241138659363,\n",
       " 0.36363634710743875,\n",
       " 0.0,\n",
       " 0.3636363305785154,\n",
       " 0.6666665925926009,\n",
       " 0.5333332977777802,\n",
       " 0.5925925706447196,\n",
       " 0.41666664930555625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5217391077504736,\n",
       " 0.6666666296296316,\n",
       " 0.0,\n",
       " 0.24999996875000394,\n",
       " 0.0,\n",
       " 0.21428570663265334,\n",
       " 0.0,\n",
       " 0.285714244897965,\n",
       " 0.0,\n",
       " 0.7333333088888897,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6999999650000017,\n",
       " 0.0,\n",
       " 0.20689654458977433,\n",
       " 0.5714285442176883,\n",
       " 0.2857142653061239,\n",
       " 0.36363634710743875,\n",
       " 0.7499999062500118,\n",
       " 0.0,\n",
       " 0.3124999902343753,\n",
       " 0.28571427210884415,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.8421052188365674,\n",
       " 0.4444444197530878,\n",
       " 0.0,\n",
       " 0.5263157756232691,\n",
       " 0.23529410380622917,\n",
       " 0.49999998214285774,\n",
       " 0.6486486311176046,\n",
       " 0.4999999750000012,\n",
       " 0.30303029384756686,\n",
       " 0.7199999712000011,\n",
       " 0.5185184993141296,\n",
       " 0.6923076656804744,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4799999808000007,\n",
       " 0.31578945706371275,\n",
       " 0.49999998214285774,\n",
       " 0.4999999750000012,\n",
       " 0.9999998750000157,\n",
       " 0.5999999700000015,\n",
       " 0.1999999900000005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38095236281179223,\n",
       " 0.0,\n",
       " 0.35714284438775556,\n",
       " 0.6666666419753096,\n",
       " 0.42857139795918586,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285442176883,\n",
       " 0.5454545206611581,\n",
       " 0.5599999776000009,\n",
       " 0.0,\n",
       " 0.42857139795918586,\n",
       " 0.0,\n",
       " 0.1999999900000005,\n",
       " 0.19999998000000202,\n",
       " 0.333333319444445,\n",
       " 0.3846153698224858,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11111110493827195,\n",
       " 0.2857142653061239,\n",
       " 0.24999996875000394,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8461538136094686,\n",
       " 0.857142734693895,\n",
       " 0.7272726611570308,\n",
       " 0.588235259515573,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666665925926009,\n",
       " 0.620689633769323,\n",
       " 0.7878787640036738,\n",
       " 0.857142734693895,\n",
       " 0.9677419042663902,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.14285713265306196,\n",
       " 0.0,\n",
       " 0.399999980000001,\n",
       " 0.47058820761245834,\n",
       " 0.7058823114186875,\n",
       " 0.4999999375000079,\n",
       " 0.0,\n",
       " 0.6285714106122454,\n",
       " 0.15384614792899431,\n",
       " 0.999999958333335,\n",
       " 0.4210526094182837,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.36363635261708027,\n",
       " 0.9999999642857155,\n",
       " 0.9999999642857155,\n",
       " 0.6086956257088858,\n",
       " 0.4347825897920613,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.639999974400001,\n",
       " 0.0,\n",
       " 0.6315789307479229,\n",
       " 0.39999998400000064,\n",
       " 0.0,\n",
       " 0.27586205945303244,\n",
       " 0.1904761859410432,\n",
       " 0.9090908264462886,\n",
       " 0.6153845917159773,\n",
       " 0.47619045351474026,\n",
       " 0.4799999808000007,\n",
       " 0.23529410380622917,\n",
       " 0.0,\n",
       " 0.8888887901234679,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.333333319444445,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9999999615384629,\n",
       " 0.4285714183673472,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.7199999712000011,\n",
       " 0.0,\n",
       " 0.6666666419753096,\n",
       " 0.5333332977777802,\n",
       " 0.6249999609375024,\n",
       " 0.9230768875739658,\n",
       " 0.7499999765625008,\n",
       " 0.27586205945303244,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42424241138659363,\n",
       " 0.64285711989796,\n",
       " 0.7777777561728402,\n",
       " 0.8695651795841226,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999998000000202,\n",
       " 0.15384614201183525,\n",
       " 0.7142856887755111,\n",
       " 0.21052630470914185,\n",
       " 0.1999999900000005,\n",
       " 0.3333333055555579,\n",
       " 0.0,\n",
       " 0.42857139795918586,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8888888641975317,\n",
       " 0.20689654458977433,\n",
       " 0.2666666488888901,\n",
       " 0.0,\n",
       " 0.2222222098765439,\n",
       " 0.0,\n",
       " 0.24999996875000394,\n",
       " 0.9999999166666736,\n",
       " 0.2222222098765439,\n",
       " 0.66666663888889,\n",
       " 0.47058820761245834,\n",
       " 0.0,\n",
       " 0.5263157617728547,\n",
       " 0.5714285510204089,\n",
       " 0.9999999444444475,\n",
       " 0.1999999900000005,\n",
       " 0.23529410380622917,\n",
       " 0.8888887901234679,\n",
       " 0.11764705190311459,\n",
       " 0.3076922840236705,\n",
       " 0.37037035665294976,\n",
       " 0.9999999166666736,\n",
       " 0.6666666419753096,\n",
       " 0.24999998437500096,\n",
       " 0.0,\n",
       " 0.27272726033057904,\n",
       " 0.6874999785156257,\n",
       " 0.6666666222222252,\n",
       " 0.0,\n",
       " 0.7586206634958391,\n",
       " 0.5263157617728547,\n",
       " 0.4999999750000012,\n",
       " 0.9999999642857155,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.37499997656250145,\n",
       " 0.21052630470914185,\n",
       " 0.0,\n",
       " 0.5714285442176883,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.44444439506173394,\n",
       " 0.27272726033057904,\n",
       " 0.4999999375000079,\n",
       " 0.0,\n",
       " 0.9999998750000157,\n",
       " 0.0,\n",
       " 0.8888887901234679,\n",
       " 0.6666666296296316,\n",
       " 0.0,\n",
       " 0.4666666511111116,\n",
       " 0.22222219753086697,\n",
       " 0.0,\n",
       " 0.6666665925926009,\n",
       " 0.6666665925926009,\n",
       " 0.4799999808000007,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8461538136094686,\n",
       " 0.6666666111111158,\n",
       " 0.23076922189349144,\n",
       " 0.4186046414277991,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38095236281179223,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.639999974400001,\n",
       " 0.0,\n",
       " 0.7333333088888897,\n",
       " 0.0,\n",
       " 0.13333332444444504,\n",
       " 0.4545454338842984,\n",
       " 0.47619045351474026,\n",
       " 0.28571427210884415,\n",
       " 0.0,\n",
       " 0.13333332444444504,\n",
       " 0.0,\n",
       " 0.4210526094182837,\n",
       " 0.6666666419753096,\n",
       " 0.0,\n",
       " 0.47619045351474026,\n",
       " 0.0,\n",
       " 0.2666666488888901,\n",
       " 0.6956521587901705,\n",
       " 0.6999999650000017,\n",
       " 0.3846153698224858,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666665277777895,\n",
       " 0.35294115570934376,\n",
       " 0.49999998437500054,\n",
       " 0.41666664930555625,\n",
       " 0.26666665777777804,\n",
       " 0.5384615177514801,\n",
       " 0.4615384437869829,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.36363634710743875,\n",
       " 0.4545454338842984,\n",
       " 0.3846153698224858,\n",
       " 0.599999940000006,\n",
       " 0.9999998750000157,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Container.summary of <keras.engine.training.Model object at 0x000001C2B9C815F8>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing.json\") as f:\n",
    "    testing_document = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_divide1(dataset,length):\n",
    "    training_set = []\n",
    "    set_count = 0\n",
    "    poc = 0\n",
    "    nec = 0\n",
    "    for question in dataset:\n",
    "        paras = document_list[question[\"docid\"]]\n",
    "        que = question[\"question\"]\n",
    "        t = {}\n",
    "        pset = []\n",
    "        for para in paras:\n",
    "            sents =  str_padding(para,seq_len)\n",
    "            for sent in sents:\n",
    "                pset.append(sent2vec(sent))\n",
    "        t[\"question\"] = [sent2vec(arr) for arr in str_padding(que,length)]\n",
    "        t[\"set\"] = pset\n",
    "        if \"text\" in question:\n",
    "            ans = question[\"text\"]\n",
    "            t[\"answer\"] = [sent2vec(arr) for arr in str_padding(ans,length)]\n",
    "        training_set.append(t)\n",
    "        set_count += 1\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_test_set = set_divide1(testing_document,seq_len)\n",
    "dev_test_set = set_divide1(dev_document,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_test_set[0][\"set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 3097\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for d in dev_test_set:\n",
    "#d = dev_test_set[1]\n",
    "    que = d[\"question\"][0]\n",
    "    X = [np.append(arr,que) for arr in d[\"set\"]]\n",
    "    X = np.array(X)\n",
    "    y = model1.predict(X)\n",
    "    index = np.argmax(y)\n",
    "    para = vec2sent(X[index])\n",
    "    ans = vec2sent(d[\"answer\"][0])\n",
    "    if ans.lower() in para.lower():\n",
    "        count +=1\n",
    "    total +=1\n",
    "    print (\"{}/{}\".format(count,total),end=\"\\r\")\n",
    "print (count,len(dev_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1952, Thomas Watson, Sr., stepped down after almost 40 years at the company helm; his son, Thomas Watson, Jr., was named president. In 1956, the company demonstrated the first practical example of artificial intelligence when Arthur L. Samuel of IBM\\'s Poughkeepsie, New York, laboratory programmed an IBM 704 not merely to play checkers but \"learn\" from its own experience. In 1957, the FORTRAN (FORmula TRANslation) scientific programming language was developed. In 1961, Thomas J. Watson, Jr.,What percentage of its desktop PCs does IBM plan to install Open Client on to?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 %'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
