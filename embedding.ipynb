{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\NLP\\QA\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from nltk.tokenize import word_tokenize\n",
    "%cd \"E:\\NLP\\QA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"documents.json\") as f:\n",
    "    doc = json.load(f)\n",
    "with open(\"training.json\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"mapping.json\",\"r\") as f:\n",
    "    word_dict = json.load(f)\n",
    "with open(\"char_dict.json\",\"r\") as f:\n",
    "    char_dict = json.load(f)\n",
    "embedding = np.load(\"embedding.npy\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98289\n",
      "100757\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for d in doc:\n",
    "    paragraphs = d[\"text\"]\n",
    "    for paragraph in paragraphs:\n",
    "        tokens += word_tokenize(paragraph.lower())\n",
    "print(len(set(tokens)))\n",
    "for sample in train:\n",
    "    question = sample[\"question\"]\n",
    "    tokens += word_tokenize(question.lower())\n",
    "print(len(set(tokens)))\n",
    "tokens = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding = []\n",
    "word_dict = dict()\n",
    "with open(\"glove.6B.50d.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        word = data[0]\n",
    "        if word in tokens:\n",
    "            embedding.append(np.array([float(i) for i in data[1:]]))\n",
    "            word_dict[word] = len(word_dict)\n",
    "embedding.append(np.random.uniform(-0.5,0.5,50))\n",
    "word_dict[\"<UNK>\"] = len(word_dict)\n",
    "embedding.append(np.zeros(50))\n",
    "word_dict[\"<PAD>\"] = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"embedding\", embedding)\n",
    "with open(\"mapping.json\",\"w\") as f:\n",
    "    json.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "char_set = []\n",
    "for token in word_dict.keys():\n",
    "    char_set += [c for c in token]\n",
    "char_set = set(char_set)\n",
    "print(len(char_set))\n",
    "char_dict = dict()\n",
    "for char in char_set:\n",
    "    char_dict[char] = len(char_dict)\n",
    "char_dict[\"<unk>\"] = len(char_dict)\n",
    "char_dict[\"<pad>\"] = len(char_dict)\n",
    "print(len(char_dict))\n",
    "with open(\"char_dict.json\",\"w\") as f:\n",
    "    json.dump(char_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72499, 50)\n",
      "72499\n",
      "216\n",
      "72498\n",
      "215\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)\n",
    "print(len(word_dict))\n",
    "print(len(char_dict))\n",
    "print(word_dict[\"<PAD>\"])\n",
    "print(char_dict[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topk = 3\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "punc = set(['\"','\\'',\"?\",\".\",\",\",\"/\",\"<\",\">\",\":\",\";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 43379/43379 [01:51<00:00, 390.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def unknown_detection(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token in word_dict:\n",
    "            new_list.append(token)\n",
    "        else:\n",
    "            new_list.append(\"<UNK>\")\n",
    "    return new_list\n",
    "\n",
    "def unknown_detection_char(char_list):\n",
    "    for i in range(len(char_list)):\n",
    "        if char_list[i] not in char_dict:\n",
    "            char_list[i] = \"<unk>\"\n",
    "    return char_list\n",
    "\n",
    "def generate_char(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token == \"<PAD>\":\n",
    "            char_list = [\"<pad>\"]*16\n",
    "        else:\n",
    "            char_list = [c for c in token[:16]]\n",
    "        while len(char_list) < 16:\n",
    "            char_list.append(\"<pad>\")\n",
    "        for char in char_list:\n",
    "            if char in char_dict:\n",
    "                new_list.append(char)\n",
    "            else:\n",
    "                new_list.append(\"<unk>\")\n",
    "    return new_list\n",
    "\n",
    "padded_train = []\n",
    "for sample in tqdm(train):\n",
    "    new_sample = dict()\n",
    "    docid = sample[\"docid\"]\n",
    "\n",
    "    question = word_tokenize(sample[\"question\"].lower())\n",
    "\n",
    "    answer = word_tokenize(sample[\"text\"].lower())\n",
    "    answer_para = sample[\"answer_paragraph\"]\n",
    "\n",
    "    para = doc[docid][\"text\"][answer_para].lower()\n",
    "    assert(doc[docid][\"docid\"] == docid)\n",
    "    para = word_tokenize(para)[:240]\n",
    "    \n",
    "    # extract indices of answer from paragraph\n",
    "    answer_idx = None\n",
    "    for i, j in enumerate(para):\n",
    "        if j == answer[0]:\n",
    "            k = 1\n",
    "            while k < len(answer) and i+k<len(para):\n",
    "                if para[i+k] != answer[k]:\n",
    "                    break\n",
    "                k += 1\n",
    "            else:\n",
    "                answer_idx = (i, i+k)\n",
    "                break\n",
    "    # ignore samples that no answer can be found\n",
    "    if answer_idx is None:\n",
    "        continue\n",
    "    \n",
    "    while len(para) < 240:\n",
    "        para.append(\"<PAD>\")\n",
    "    content_char = generate_char(para)\n",
    "    content = unknown_detection(para)\n",
    "    \n",
    "    while len(answer) < 7:\n",
    "        answer.append(\"<PAD>\")\n",
    "    answer = answer[:7]\n",
    "    answer_char = generate_char(answer)\n",
    "    answer = unknown_detection(answer)\n",
    "    \n",
    "    padded_question = question[:30]\n",
    "    while len(padded_question) < 30:\n",
    "        padded_question.append(\"<PAD>\")\n",
    "    question_char = generate_char(padded_question)\n",
    "    padded_question = unknown_detection(padded_question)\n",
    "    \n",
    "    new_sample[\"question\"] = padded_question\n",
    "    new_sample[\"q_char\"] = question_char\n",
    "    new_sample[\"content\"] = content\n",
    "    new_sample[\"c_char\"] = content_char\n",
    "    new_sample[\"answer\"] = answer\n",
    "    new_sample[\"answer_char\"] = answer_char\n",
    "    new_sample[\"answer_idx\"] = answer_idx\n",
    "    \n",
    "    assert len(padded_question) == 30\n",
    "    assert len(question_char) == 480\n",
    "    assert len(content) == 240\n",
    "    assert len(content_char) == 3840\n",
    "    assert len(answer) == 7\n",
    "    assert len(answer_char) == 112\n",
    "    assert len(answer_idx) == 2\n",
    "    \n",
    "    padded_train.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['q_char', 'answer', 'answer_idx', 'question', 'content', 'answer_char', 'c_char'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[888].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cylinder']\n",
      "['cylinder', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[76, 77]\n"
     ]
    }
   ],
   "source": [
    "curr = padded_train[1]\n",
    "start, end = curr[\"answer_idx\"]\n",
    "print(curr[\"content\"][start:end])\n",
    "print(curr[\"answer\"])\n",
    "print(list(curr[\"answer_idx\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(padded):\n",
    "    \"\"\"\n",
    "    input for NN:\n",
    "        c:           content token\n",
    "        c_char:      content character\n",
    "        q:           question token\n",
    "        q_char:      question character\n",
    "    output:\n",
    "        probability distribution of start and end position over content\n",
    "    \"\"\"\n",
    "    c, c_chars, q, q_chars, a_idx = [], [], [], [], []\n",
    "    \n",
    "    cnt = 0\n",
    "    for sample in tqdm(padded):\n",
    "        cnt += 1\n",
    "        question = sample[\"question\"]\n",
    "        content = sample[\"content\"]\n",
    "        q_char = sample[\"q_char\"]\n",
    "        c_char = sample[\"c_char\"]\n",
    "        aidx = sample[\"answer_idx\"]\n",
    "        answer = sample[\"answer\"]\n",
    "        \n",
    "        # ignore answer only contains <UNK>\n",
    "        idx = answer.index(\"<PAD>\") if \"<PAD>\" in answer else 7\n",
    "        if all(t == \"<UNK>\" for t in answer[:idx]):\n",
    "            continue\n",
    "        \n",
    "        q_mapped = [word_dict[t] for t in question]\n",
    "        c_mapped = [word_dict[t] for t in content]\n",
    "        q_char_mapped = [char_dict[ch] for ch in q_char]\n",
    "        c_char_mapped = [char_dict[ch] for ch in c_char]\n",
    "        \n",
    "        c.append(c_mapped)\n",
    "        q.append(q_mapped)\n",
    "        c_chars.append(c_char_mapped)\n",
    "        q_chars.append(q_char_mapped)\n",
    "        a_idx.append(aidx)\n",
    "        \n",
    "    return np.array(c), np.array(c_chars), np.array(q), np.array(q_chars), np.array(a_idx)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_score(pred_s, pred_e, true_s, true_e, context):\n",
    "    # computes average f_measure for a batch\n",
    "    f_sum = 0\n",
    "    l = len(pred_s)\n",
    "    for i in range(l):\n",
    "        if pred_e[i] < pred_s[i]:\n",
    "            continue\n",
    "        TP, FN, FP = 0, 0, 0\n",
    "        guess = context[i][pred_s[i]:pred_e[i]+1]\n",
    "        true = context[i][true_s[i]:true_e[i]+1]\n",
    "        for token in guess:\n",
    "            if token in true:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        for token in true:\n",
    "            if token not in guess:\n",
    "                FN += 1\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        f = 2*precision*recall/(precision+recall+1e-8)\n",
    "        f_sum += f\n",
    "    return f_sum/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 41802/41802 [00:17<00:00, 2428.02it/s]\n"
     ]
    }
   ],
   "source": [
    "c, c_char, q, q_char, a_idx = generate_training_data(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 26]\n"
     ]
    }
   ],
   "source": [
    "print(a_idx[30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "c_ph = tf.placeholder(tf.int32, c.shape)\n",
    "c_char_ph = tf.placeholder(tf.int32, c_char.shape)\n",
    "q_ph = tf.placeholder(tf.int32, q.shape)\n",
    "q_char_ph = tf.placeholder(tf.int32, q_char.shape)\n",
    "aidx_ph = tf.placeholder(tf.int32, a_idx.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((c_ph, c_char_ph, q_ph, q_char_ph, aidx_ph))\n",
    "\n",
    "epoch = 20\n",
    "batch = 64\n",
    "\n",
    "def make_dataset(dataset):\n",
    "    dataset = dataset.shuffle(40109)\n",
    "    dataset = dataset.repeat(epoch)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(batch*2)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n",
    "\n",
    "train_iter = make_dataset(dataset)\n",
    "next_batch = train_iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: depthwise separable convolutions\n",
    "# todo: position encoding\n",
    "# todo: multihead attention(maybe)\n",
    "# todo: regularization(dropout)\n",
    "\n",
    "def embedding_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        #convolution block\n",
    "        residual1 = tf.layers.conv1d(inputs, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        \n",
    "        norm1 = tf.contrib.layers.layer_norm(residual1)\n",
    "        conv2 = tf.layers.conv1d(norm1, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        residual2 = tf.add(residual1, conv2)\n",
    "        \n",
    "        norm2 = tf.contrib.layers.layer_norm(residual2)\n",
    "        conv3 = tf.layers.conv1d(norm2, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        residual3 = tf.add(residual2, conv3)\n",
    "        \n",
    "        norm3 = tf.contrib.layers.layer_norm(residual3)\n",
    "        conv4 = tf.layers.conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        residual4 = tf.add(residual3, conv4)\n",
    "        \n",
    "        # self-attention block\n",
    "        norm4 = tf.contrib.layers.layer_norm(residual4)\n",
    "        attention = tf.matmul(norm4, norm4, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(norm4)[-1], dtype=tf.float32)\n",
    "        scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "        attention = tf.nn.softmax(scaled, axis=-1)\n",
    "        attention_out = tf.matmul(attention, norm4)\n",
    "        residual5 = tf.add(residual4, attention_out)\n",
    "        \n",
    "        # feedforwoad layer\n",
    "        norm5 = tf.contrib.layers.layer_norm(residual5)\n",
    "        ffn1 = tf.layers.conv1d(norm5, 128, 1, activation=tf.nn.relu)\n",
    "        ffn2 = tf.layers.conv1d(ffn1, 128, 1)\n",
    "        residual6 = tf.add(residual5, ffn2)\n",
    "    return residual6\n",
    "\n",
    "def model_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        outputs = inputs\n",
    "        for i in range(7):\n",
    "            with tf.variable_scope(\"conv_block{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm1 = tf.contrib.layers.layer_norm(outputs)\n",
    "                conv1 = tf.layers.conv1d(norm1, 128, 7, padding=\"same\", activation=tf.nn.relu)\n",
    "                residual1 = tf.add(outputs, conv1)\n",
    "            \n",
    "            with tf.variable_scope(\"self_attention{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm2 = tf.contrib.layers.layer_norm(residual1)\n",
    "                attention = tf.matmul(norm2, norm2, transpose_b=True)\n",
    "                dk = tf.cast(tf.shape(norm2)[-1], dtype=tf.float32)\n",
    "                scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "                attention = tf.nn.softmax(scaled, axis=-1)\n",
    "                attention_out = tf.matmul(attention, norm2)\n",
    "                residual2 = tf.add(residual1, attention_out)\n",
    "            \n",
    "            with tf.variable_scope(\"feedforward{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm3 = tf.contrib.layers.layer_norm(residual2)\n",
    "                ffn1 = tf.layers.conv1d(norm3, 128, 1, activation=tf.nn.relu)\n",
    "                ffn2 = tf.layers.conv1d(ffn1, 128, 1)\n",
    "                outputs = tf.add(residual2, ffn2)\n",
    "    return outputs\n",
    "\n",
    "def highway(scope, inputs):\n",
    "    size = inputs.shape.as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        T1 = tf.layers.conv1d(inputs, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H1 = tf.layers.conv1d(inputs, size, 1)\n",
    "        highway1 = T1 * H1 + inputs * (1.0 - T1)\n",
    "        T2 = tf.layers.conv1d(highway1, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H2 = tf.layers.conv1d(highway1, size, 1)\n",
    "        highway2 = T2 * H2 + highway1 * (1.0 - T2)\n",
    "    return highway2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30, 128) (?, 240, 128)\n",
      "(?, 240, 512)\n",
      "(?, 240, 128) (?, 240, 128) (?, 240, 128)\n",
      "(?,) (?,)\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    q_input = tf.placeholder(tf.int32, [None, 30], name=\"q\")\n",
    "    q_char_input = tf.placeholder(tf.int32, [None, 480], name=\"q_char\")\n",
    "    c_input = tf.placeholder(tf.int32, [None, 240], name=\"c\")\n",
    "    c_char_input = tf.placeholder(tf.int32, [None, 3840], name=\"c_char\")\n",
    "    start_mask = tf.placeholder(tf.int32, [None], name=\"start_mask\")\n",
    "    end_mask = tf.placeholder(tf.int32, [None], name=\"end_mask\")\n",
    "    batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "\n",
    "tf.add_to_collection(\"infer_input\", q_input)\n",
    "tf.add_to_collection(\"infer_input\", q_char_input)\n",
    "tf.add_to_collection(\"infer_input\", c_input)\n",
    "tf.add_to_collection(\"infer_input\", c_char_input)\n",
    "\n",
    "with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "    # input embedding layer\n",
    "    with tf.variable_scope(\"W_Embedding\"):\n",
    "        pretrained_embedding = tf.get_variable(\"w_embedding\",\n",
    "                                               shape=[72497, 50],\n",
    "                                               initializer=tf.constant_initializer(embedding[:-2,:]),\n",
    "                                               trainable=False)\n",
    "        unknown_embedding = tf.get_variable(\"unknown\",\n",
    "                                            shape=[1, 50],\n",
    "                                            initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                            trainable=True)\n",
    "        padding_embedding = tf.get_variable(\"padding\",\n",
    "                                            shape=[1, 50],\n",
    "                                            initializer=tf.zeros_initializer(),\n",
    "                                            trainable=False)\n",
    "        word_embedding = tf.concat([pretrained_embedding, unknown_embedding, padding_embedding], 0)\n",
    "        q_embed = tf.nn.embedding_lookup(word_embedding, q_input)\n",
    "        c_embed = tf.nn.embedding_lookup(word_embedding, c_input)\n",
    "\n",
    "    with tf.variable_scope(\"C_Embedding\"):\n",
    "        char_embedding = tf.get_variable(\"c_embedding\",\n",
    "                                         shape=[215, 200],\n",
    "                                         initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                         trainable=True)\n",
    "        padding = tf.get_variable(\"padding\",\n",
    "                                  shape=[1, 200],\n",
    "                                  initializer=tf.zeros_initializer(),\n",
    "                                  trainable=False)\n",
    "        char_combined = tf.concat([char_embedding, padding], 0, name=\"char_embedding\")\n",
    "        q_char_embed = tf.nn.embedding_lookup(char_combined, q_char_input)\n",
    "        c_char_embed = tf.nn.embedding_lookup(char_combined, c_char_input)\n",
    "        squeeze_to_word_q = tf.layers.max_pooling1d(q_char_embed, 16, 16)\n",
    "        squeeze_to_word_c = tf.layers.max_pooling1d(c_char_embed, 16, 16)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_output\"):\n",
    "        q_embed_out = tf.concat([q_embed, squeeze_to_word_q], 2)\n",
    "        c_embed_out = tf.concat([c_embed, squeeze_to_word_c], 2)\n",
    "        q_embed_out = highway(\"highway\", q_embed_out)\n",
    "        c_embed_out = highway(\"highway\", c_embed_out)\n",
    "\n",
    "with tf.variable_scope(\"Embedding_Encoder_Layer\"):\n",
    "    # embedding encoder layer\n",
    "    q_encoded = embedding_encoder_block(\"encoder_block\", q_embed_out)\n",
    "    c_encoded = embedding_encoder_block(\"encoder_block\", c_embed_out)\n",
    "    print(q_encoded.shape, c_encoded.shape)\n",
    "    \n",
    "with tf.variable_scope(\"Context_Query_Attention_Layer\"):\n",
    "    # context_query attention layer\n",
    "    # first compute similarity matrix between context and query\n",
    "    # S_tj = w * [C_t; Q_j; C_t*Q_j]\n",
    "    c_expand = tf.expand_dims(c_encoded, 2)\n",
    "    c_expand = tf.tile(c_expand, [1,1,30,1])\n",
    "    q_expand = tf.expand_dims(q_encoded, 1)\n",
    "    q_expand = tf.tile(q_expand, [1,240,1,1])\n",
    "    qc_mul = tf.multiply(c_expand, q_expand)\n",
    "    concat = tf.concat([c_expand,q_expand,qc_mul], 3)\n",
    "    w = tf.get_variable(\"s_w\", [384,1])\n",
    "    \n",
    "    # similarity matrix S (logits)\n",
    "    S = tf.einsum(\"abcde,ef->abcdf\", tf.expand_dims(concat,3),w)\n",
    "    S = tf.squeeze(S,[-2,-1])\n",
    "    # S_: softmax over rows\n",
    "    S_ = tf.nn.softmax(S)\n",
    "    # S__T: transpose of softmax over coloum\n",
    "    S__T = tf.transpose(tf.nn.softmax(S, axis=1),[0,2,1])\n",
    "    # context_query attention\n",
    "    A = tf.matmul(S_, q_encoded)\n",
    "    # query_context attention\n",
    "    B = tf.matmul(tf.matmul(S_, S__T), c_encoded)\n",
    "    \n",
    "    # layer output\n",
    "    G = tf.concat([c_encoded, A, tf.multiply(c_encoded,A), tf.multiply(c_encoded,B)],2)\n",
    "    print(G.shape)\n",
    "\n",
    "with tf.variable_scope(\"Model_Encoder_Layer\"):\n",
    "    # model encoder layer\n",
    "    G_conv = tf.layers.conv1d(G, 128, 7, padding=\"same\", activation=tf.nn.relu)\n",
    "    model_encoder1 = model_encoder_block(\"model_encoder\", G_conv)\n",
    "    model_encoder2 = model_encoder_block(\"model_encoder\", model_encoder1)\n",
    "    model_encoder3 = model_encoder_block(\"model_encoder\", model_encoder2)\n",
    "    print(model_encoder1.shape,model_encoder2.shape,model_encoder3.shape)\n",
    "\n",
    "global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    # output layer\n",
    "    p1_input = tf.concat([model_encoder1, model_encoder2],2)\n",
    "    p2_input = tf.concat([model_encoder2, model_encoder3],2)\n",
    "    p1_prob = tf.nn.softmax(tf.squeeze(tf.layers.conv1d(p1_input, 1, 1),-1))\n",
    "    p2_prob = tf.nn.softmax(tf.squeeze(tf.layers.conv1d(p2_input, 1, 1),-1))\n",
    "    pred_s = tf.argmax(p1_prob, axis=1)\n",
    "    pred_e = tf.argmax(p2_prob, axis=1)\n",
    "    s_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(start_mask,1)],1)\n",
    "    e_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(end_mask,1)],1)\n",
    "    yhat_p1 = tf.add(tf.gather_nd(p1_prob, s_pairs), 1e-15)\n",
    "    yhat_p2 = tf.add(tf.gather_nd(p2_prob, e_pairs), 1e-15)\n",
    "\n",
    "tf.add_to_collection(\"predictions\", p1_prob)\n",
    "tf.add_to_collection(\"predictions\", p2_prob)\n",
    "    \n",
    "with tf.variable_scope(\"Optimizer\"):\n",
    "    # add l2 weight decay to all variables\n",
    "    trainables = tf.trainable_variables()\n",
    "    loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in trainables if 'bias' not in v.name ]) * 3e-7\n",
    "    loss = -tf.reduce_mean(tf.log(yhat_p1) + tf.log(yhat_p2)) + loss_l2\n",
    "    \n",
    "    # apply exponential moving average\n",
    "    opt_op = tf.train.AdamOptimizer(beta1=0.8,epsilon=1e-7).minimize(loss, global_step=global_step)\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "    with tf.control_dependencies([opt_op]):\n",
    "        train_step = ema.apply(trainables)\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "f_measure = tf.get_variable(\"f_measure\", ())\n",
    "tf.summary.scalar(\"f_measure\", f_measure)\n",
    "print(yhat_p1.shape, yhat_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(train_iter.initializer, feed_dict={c_ph: c,\n",
    "                                                    c_char_ph: c_char,\n",
    "                                                    q_ph: q,\n",
    "                                                    q_char_ph: q_char,\n",
    "                                                    aidx_ph: a_idx})\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=3)\n",
    "        cnt = 0\n",
    "        f = 0\n",
    "        while True:\n",
    "            try:\n",
    "                cnt += 1\n",
    "                next_c, next_c_char, next_q, next_q_char, next_mask = sess.run(next_batch)\n",
    "                next_smask = next_mask[:,0]\n",
    "                next_emask = next_mask[:,1]-1\n",
    "                feed_dict = {q_input: next_q,\n",
    "                             q_char_input: next_q_char,\n",
    "                             c_input: next_c,\n",
    "                             c_char_input: next_c_char,\n",
    "                             start_mask: next_smask,\n",
    "                             end_mask: next_emask,\n",
    "                             f_measure:f,\n",
    "                             batch_size: len(next_c)}\n",
    "                _, ps, pe, step, s = sess.run([train_step, pred_s, pred_e, global_step, merged],feed_dict=feed_dict)\n",
    "                f = f_score(ps, pe, next_smask, next_emask, next_c)\n",
    "                writer.add_summary(s, step)\n",
    "                if cnt % 500 == 0:\n",
    "                    print(cnt)\n",
    "                    saver.save(sess, \"model/naive\", global_step=step)\n",
    "            except tf.errors.InvalidArgumentError:\n",
    "                ec = next_c\n",
    "                ecc = next_c_char\n",
    "                eq = next_q\n",
    "                eqc = next_q_char\n",
    "                em = next_mask\n",
    "                print(\"wrong\")\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                saver.save(sess, \"model/naive\", global_step=step)\n",
    "        print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    next_smask = em[:,0]\n",
    "    next_emask = em[:,1]-1\n",
    "    feed_dict = {q_input: eq,\n",
    "                 q_char_input: eqc,\n",
    "                 c_input: ec,\n",
    "                 c_char_input: ecc,\n",
    "                 start_mask: next_smask,\n",
    "                 end_mask: next_emask,\n",
    "                 f_measure:f,\n",
    "                 batch_size: len(next_c)}\n",
    "    _, ps, pe, step = sess.run([train_step, s_pairs, e_pairs, global_step],feed_dict=feed_dict)\n",
    "    #f = f_score(ps, pe, next_smask, next_emask, next_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "a-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
