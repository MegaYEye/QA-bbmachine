{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to evaluate IR model and deep learning model altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"documents.json\") as f:\n",
    "    doc = json.load(f)\n",
    "with open(\"devel.json\") as f:\n",
    "    dev = json.load(f)\n",
    "with open(\"word_dict.json\") as f:\n",
    "    word_dict = json.load(f)\n",
    "with open(\"char_dict.json\") as f:\n",
    "    char_dict = json.load(f)\n",
    "embedding = np.load(\"embedding.npy\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use double TF-IDF to choose _topk-s_ sentences from _topk-p_ paragraphs. Then merge these sentences to form an input paragraph for deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topk_p = 3\n",
    "topk_s = 5\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "punc = set(['\"','\\'',\"?\",\".\",\",\",\"/\",\"<\",\">\",\":\",\";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_detection(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token in word_dict:\n",
    "            new_list.append(token)\n",
    "        else:\n",
    "            new_list.append(\"<UNK>\")\n",
    "    return new_list\n",
    "\n",
    "def generate_char(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token == \"<PAD>\":\n",
    "            char_list = [\"<pad>\"]*16\n",
    "        else:\n",
    "            char_list = [c for c in token[:16]]\n",
    "        while len(char_list) < 16:\n",
    "            char_list.append(\"<pad>\")\n",
    "        for char in char_list:\n",
    "            if char in char_dict:\n",
    "                new_list.append(char)\n",
    "            else:\n",
    "                new_list.append(\"<unk>\")\n",
    "    assert len(new_list) == len(token_list) * 16\n",
    "    return new_list\n",
    "\n",
    "if os.path.exists(\"tfidfs.pickle\"):\n",
    "    with open(\"tfidfs.pickle\",\"rb\") as f:\n",
    "        tfidfs = pickle.load(f)\n",
    "    tqdm.write(\"matrices loaded\")\n",
    "else:\n",
    "    tfidfs = dict()\n",
    "    for d in doc:\n",
    "        tfidf = TfidfVectorizer(tokenizer=word_tokenize,\n",
    "                                stop_words='english',\n",
    "                                max_df=0.5,\n",
    "                                smooth_idf=False,\n",
    "                                sublinear_tf=True)\n",
    "        paragraphs = [p.lower() for p in d[\"text\"]]\n",
    "        res = tfidf.fit_transform(paragraphs).toarray()\n",
    "        mapping = tfidf.vocabulary_\n",
    "        tfidfs[d[\"docid\"]] = [res, mapping]\n",
    "    with open(\"tfidfs.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tfidfs, f)\n",
    "    tqdm.write(\"matrices building complete\")\n",
    "\n",
    "topk_p = 3\n",
    "topk_s = 5\n",
    "\n",
    "padded_dev = []\n",
    "for sample in tqdm(dev):\n",
    "    new_sample = dict()\n",
    "    \n",
    "    docid = sample[\"docid\"]\n",
    "    answer = word_tokenize(sample[\"text\"])\n",
    "\n",
    "    question = word_tokenize(sample[\"question\"].lower().strip())\n",
    "    rmed = []\n",
    "    for token in question:\n",
    "        if token not in stopword and token not in punc:\n",
    "            rmed.append(token)\n",
    "    question = rmed\n",
    "    \n",
    "    res, mapping = tfidfs[docid]\n",
    "    # set accumulator for each paragraph\n",
    "    a_d = [0 for _ in range(res.shape[0])]\n",
    "    for token in question:\n",
    "        for i in range(len(a_d)):\n",
    "            if token in mapping:\n",
    "                a_d[i] += res[i, mapping[token]]\n",
    "\n",
    "    k = topk_p if res.shape[0] > topk_p else res.shape[0]\n",
    "    pred = np.argpartition(a_d, -k)[-k:]\n",
    "    pred = set(pred)\n",
    "    combined = []\n",
    "    for idx in pred:\n",
    "        sents = sent_tokenize(doc[docid][\"text\"][idx])\n",
    "        for s in sents:\n",
    "            combined.append(s.lower())\n",
    "\n",
    "    # rank sentences in combined sents\n",
    "    tfidf = TfidfVectorizer(smooth_idf=False,\n",
    "                            sublinear_tf=True,\n",
    "                            tokenizer=word_tokenize)\n",
    "    array = tfidf.fit_transform(combined).toarray()\n",
    "    mapping = tfidf.vocabulary_\n",
    "\n",
    "    a_d = np.zeros(len(combined))\n",
    "    for token in question:\n",
    "        for i in range(len(a_d)):\n",
    "            if token in mapping:\n",
    "                a_d[i] += array[i, mapping[token]]\n",
    "    # return top k results\n",
    "    k = topk_s if len(combined) > topk_s else len(combined)\n",
    "    pred = np.argpartition(a_d, -k)[-k:]\n",
    "    pred = pred[np.argsort(a_d[pred])].tolist()\n",
    "    \n",
    "    para = []\n",
    "    while len(para) < 240 and len(pred) > 0:\n",
    "        idx = pred.pop()\n",
    "        sent = word_tokenize(combined[idx])[:80]\n",
    "        l = len(sent)\n",
    "        if len(para) + l <= 240:\n",
    "            para += sent\n",
    "    \n",
    "    content_char = generate_char(para)\n",
    "    content = unknown_detection(para)\n",
    "        \n",
    "    padded_question = word_tokenize(sample[\"question\"].lower())[:30]\n",
    "    while len(padded_question) < 30:\n",
    "        padded_question.append(\"<PAD>\")\n",
    "    question_char = generate_char(padded_question)\n",
    "    padded_question = unknown_detection(padded_question)\n",
    "    \n",
    "    new_sample[\"question\"] = padded_question\n",
    "    new_sample[\"q_char\"] = question_char\n",
    "    new_sample[\"content\"] = content\n",
    "    new_sample[\"c_char\"] = content_char\n",
    "    new_sample[\"answer\"] = answer\n",
    "    \n",
    "    assert len(padded_question) == 30\n",
    "    assert len(question_char) == 480\n",
    "    assert len(content) <= 240\n",
    "    assert len(content_char) <= 3840\n",
    "    assert len(content_char) == len(content) * 16\n",
    "    \n",
    "    padded_dev.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_data(padded):\n",
    "\n",
    "    c, c_chars, q, q_chars, answer = [], [], [], [], []\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in tqdm(range(len(padded))):\n",
    "        cnt += 1\n",
    "        sample = padded[i]\n",
    "        question = sample[\"question\"]\n",
    "        content = sample[\"content\"]\n",
    "        q_char = sample[\"q_char\"]\n",
    "        c_char = sample[\"c_char\"]\n",
    "        a = sample[\"answer\"]\n",
    "        \n",
    "        q_mapped = [word_dict[t] for t in question]\n",
    "        c_mapped = [word_dict[t] for t in content]\n",
    "        q_char_mapped = [char_dict[ch] for ch in q_char]\n",
    "        c_char_mapped = [char_dict[ch] for ch in c_char]\n",
    "        \n",
    "        c_mapped = tf.keras.preprocessing.sequence.pad_sequences([c_mapped], maxlen=240, padding=\"post\",value=word_dict[\"<PAD>\"])[0]\n",
    "        c_char_mapped = tf.keras.preprocessing.sequence.pad_sequences([c_char_mapped], maxlen=3840, padding=\"post\",value=char_dict[\"<pad>\"])[0]\n",
    "        \n",
    "        c.append(c_mapped)\n",
    "        q.append(q_mapped)\n",
    "        c_chars.append(c_char_mapped)\n",
    "        q_chars.append(q_char_mapped)\n",
    "        answer.append(a)\n",
    "        \n",
    "    return np.array(c), np.array(c_chars), np.array(q), np.array(q_chars), answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, c_char, q, q_char, answer = generate_input_data(padded_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.shape, c_char.shape)\n",
    "print(q.shape, q_char.shape)\n",
    "print(len(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator to feed batches into DL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices = np.arange(len(c))\n",
    "def dev_batch(batch=16):\n",
    "    np.random.shuffle(train_indices)\n",
    "    for i in range(int(math.ceil(len(c)/batch))):\n",
    "        start_index = (i*batch)%len(c)\n",
    "        idx = train_indices[start_index:start_index+batch]\n",
    "        c_b = c[idx]\n",
    "        c_char_b = c_char[idx]\n",
    "        q_b = q[idx]\n",
    "        q_char_b = q_char[idx]\n",
    "        a_b = []\n",
    "        for j in idx:\n",
    "            a_b.append(answer[j])\n",
    "        yield c_b, c_char_b, q_b, q_char_b, a_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for calculating f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown_detection(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token in word_dict:\n",
    "            new_list.append(token)\n",
    "        else:\n",
    "            new_list.append(\"<UNK>\")\n",
    "    return new_list\n",
    "\n",
    "def f_dev(pred_s, pred_e, a, context):\n",
    "    # computes average f_measure for a batch\n",
    "    f_sum = 0\n",
    "    l = len(pred_s)\n",
    "    for i in range(l):\n",
    "        pair, _ = prob_dp(pred_s[i], pred_e[i])\n",
    "        s_i, e_i = pair\n",
    "        if e_i < s_i:\n",
    "            continue\n",
    "        TP, FN, FP = 0, 0, 0\n",
    "        guess = context[i][s_i:e_i+1]\n",
    "        true = [word_dict[t] for t in unknown_detection(a[i])]\n",
    "        for token in guess:\n",
    "            if token in true:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        for token in true:\n",
    "            if token not in guess:\n",
    "                FN += 1\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        f = 2*precision*recall/(precision+recall+1e-8)\n",
    "        f_sum += f\n",
    "    return f_sum/l\n",
    "\n",
    "def prob_dp(set1,set2):\n",
    "    assert len(set1) == len(set2)\n",
    "    max1 = 0\n",
    "    maxi1 = 0\n",
    "    maxpair = None\n",
    "    maxp = 0\n",
    "    for i in range(len(set1)):\n",
    "        if set1[i]>max1:\n",
    "            max1 = set1[i]\n",
    "            maxi1 = i\n",
    "        if max1 * set2[i] > maxp:\n",
    "            maxp = max1 * set2[i]\n",
    "            maxpair = [maxi1,i]\n",
    "    assert maxpair[0] <= maxpair[1]\n",
    "    return maxpair,maxp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the same computation graph so that we can load weights in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def embedding_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        # first encode input with position info\n",
    "        pos_encoded = position_encoding(inputs)\n",
    "        # project input to dimension 128\n",
    "        residual1 = tf.layers.separable_conv1d(pos_encoded, 128, 1, padding=\"same\",activation=tf.nn.relu)\n",
    "        \n",
    "        #convolution block\n",
    "        norm1 = tf.contrib.layers.layer_norm(residual1)\n",
    "        norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "        conv1 = tf.layers.separable_conv1d(norm1, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "        residual2 = tf.add(residual1, conv1)\n",
    "        \n",
    "        norm2 = tf.contrib.layers.layer_norm(residual2)\n",
    "        norm2 = tf.nn.dropout(norm2, 1-dp)\n",
    "        conv2 = tf.layers.separable_conv1d(norm2, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv2 = tf.nn.dropout(conv2, 1-dp)\n",
    "        residual3 = tf.add(residual2, conv2)\n",
    "        \n",
    "        norm3 = tf.contrib.layers.layer_norm(residual3)\n",
    "        norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "        conv3 = tf.layers.separable_conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv3 = tf.nn.dropout(conv3, 1-dp)\n",
    "        residual4 = tf.add(residual3, conv3)\n",
    "        \n",
    "        norm4 = tf.contrib.layers.layer_norm(residual4)\n",
    "        norm4 = tf.nn.dropout(norm4, 1-dp)\n",
    "        conv4 = tf.layers.separable_conv1d(norm4, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv4 = tf.nn.dropout(conv4, 1-dp)\n",
    "        residual5 = tf.add(residual4, conv4)\n",
    "        \n",
    "        # self-attention block\n",
    "        norm5 = tf.contrib.layers.layer_norm(residual5)\n",
    "        norm5 = tf.nn.dropout(norm5, 1-dp)\n",
    "        attention_out = multihead_self_attention(norm5, \"self_attention\")\n",
    "        attention_out = tf.nn.dropout(attention_out, 1-dp)\n",
    "        residual6 = tf.add(residual5, attention_out)\n",
    "        \n",
    "        # feedforwoad layer\n",
    "        norm6 = tf.contrib.layers.layer_norm(residual6)\n",
    "        norm6 = tf.nn.dropout(norm6, 1-dp)\n",
    "        ffn1 = tf.layers.separable_conv1d(norm6, 128, 1, activation=tf.nn.relu)\n",
    "        ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "        ffn2 = tf.layers.separable_conv1d(ffn1, 128, 1)\n",
    "        ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "        residual7 = tf.add(residual6, ffn2)\n",
    "    return residual7\n",
    "\n",
    "def model_encoder_block(scope, inputs, projection=False):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        inputs = position_encoding(inputs)\n",
    "        if projection:\n",
    "            outputs = tf.layers.separable_conv1d(inputs, 128, 1, padding=\"same\", activation=tf.nn.relu)\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        for i in range(7):\n",
    "            with tf.variable_scope(\"conv_block{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm0 = tf.contrib.layers.layer_norm(outputs)\n",
    "                norm0 = tf.nn.dropout(norm0, 1-dp)\n",
    "                conv0 = tf.layers.separable_conv1d(norm0, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv0 = tf.nn.dropout(conv0, 1-dp)\n",
    "                residual0 = tf.add(outputs, conv0)\n",
    "                \n",
    "                norm1 = tf.contrib.layers.layer_norm(residual0)\n",
    "                norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "                conv1 = tf.layers.separable_conv1d(norm1, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "                residual1 = tf.add(residual0, conv1)\n",
    "            \n",
    "            with tf.variable_scope(\"self_attention{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm2 = tf.contrib.layers.layer_norm(residual1)\n",
    "                norm2 = tf.nn.dropout(norm2, 1-dp)\n",
    "                attention_out = multihead_self_attention(norm2, \"self_attention\")\n",
    "                attention_out = tf.nn.dropout(attention_out, 1-dp)\n",
    "                residual2 = tf.add(residual1, attention_out)\n",
    "            \n",
    "            with tf.variable_scope(\"feedforward{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm3 = tf.contrib.layers.layer_norm(residual2)\n",
    "                norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "                ffn1 = tf.layers.separable_conv1d(norm3, 128, 1, activation=tf.nn.relu)\n",
    "                ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "                ffn2 = tf.layers.separable_conv1d(ffn1, 128, 1)\n",
    "                ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "                outputs = tf.add(residual2, ffn2)\n",
    "    return outputs\n",
    "\n",
    "def highway(scope, inputs):\n",
    "    # two layer highway network\n",
    "    size = inputs.shape.as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        T1 = tf.layers.separable_conv1d(inputs, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H1 = tf.layers.separable_conv1d(inputs, size, 1, activation=tf.nn.relu)\n",
    "        H1 = tf.nn.dropout(H1, 1-dp)\n",
    "        highway1 = T1 * H1 + inputs * (1.0 - T1)\n",
    "        \n",
    "        T2 = tf.layers.separable_conv1d(highway1, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H2 = tf.layers.separable_conv1d(highway1, size, 1, activation=tf.nn.relu)\n",
    "        H2 = tf.nn.dropout(H2, 1-dp)\n",
    "        highway2 = T2 * H2 + highway1 * (1.0 - T2)\n",
    "    return highway2\n",
    "\n",
    "def multihead_self_attention(inputs, scope, heads=8):\n",
    "    # restricted multi-head self-attention\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        depth = inputs.get_shape().as_list()[-1]\n",
    "        WQ = tf.get_variable(\"WQ\", [depth, depth])\n",
    "        WK = tf.get_variable(\"WK\", [depth, depth])\n",
    "        WV = tf.get_variable(\"WV\", [depth, depth])\n",
    "        \n",
    "        Q = tf.einsum(\"bsd,dw->bsw\", inputs, WQ)\n",
    "        K = tf.einsum(\"bsd,dw->bsw\", inputs, WK)\n",
    "        V = tf.einsum(\"bsd,dw->bsw\", inputs, WV) # [batch, sequence_length, depth]\n",
    "        \n",
    "        # split shape for vectorization\n",
    "        Q_ = tf.concat(tf.split(Q, heads, axis=2), axis=0)\n",
    "        K_ = tf.concat(tf.split(K, heads, axis=2), axis=0)\n",
    "        V_ = tf.concat(tf.split(V, heads, axis=2), axis=0) # [8 * batch, sequence_length, depth / 8]\n",
    "        \n",
    "        attention_logits = tf.matmul(Q_, K_, transpose_b=True)\n",
    "        dk = depth / heads\n",
    "        scaled = tf.divide(attention_logits, tf.sqrt(dk))\n",
    "        attention = tf.nn.softmax(scaled, axis=-1)\n",
    "        attention_out = tf.matmul(attention, V_)\n",
    "        \n",
    "        # retrieve shape\n",
    "        attention_out = tf.concat(tf.split(attention_out, heads, axis=0), axis=2)\n",
    "    return attention_out\n",
    "\n",
    "def position_encoding(inputs):\n",
    "    \"\"\"\n",
    "    sinusoids position encoding\n",
    "    from One Model to Learn Then All\n",
    "    -- input: [None, sequence_length, depth]\n",
    "    -- output: [None, sequence_length, depth]\n",
    "    \"\"\"\n",
    "    _, seq_length, depth = inputs.get_shape().as_list()\n",
    "    pos_encoding = np.array([\n",
    "        [pos * np.power(1e-4, -(i//2)*2/depth) for i in range(depth)]\n",
    "        for pos in range(seq_length)])\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "    pos_encoding = tf.convert_to_tensor(pos_encoding, tf.float32)\n",
    "    return inputs+pos_encoding\n",
    "\n",
    "def query_context_co_attention(w, inputs, context, query):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        w: similarity funciton weight\n",
    "        inputs: [q, c, q*c]\n",
    "    output:\n",
    "        A: context-to-query attention\n",
    "        B: query-to-context attention\n",
    "    \"\"\"\n",
    "    # similarity matrix S (logits)\n",
    "    S = tf.einsum(\"abcde,ef->abcdf\", tf.expand_dims(inputs,3),w)\n",
    "    S = tf.squeeze(S,[-2,-1])\n",
    "    # S_: softmax over rows\n",
    "    S_ = tf.nn.softmax(S)\n",
    "    # S__T: transpose of softmax over coloum\n",
    "    S__T = tf.transpose(tf.nn.softmax(S, axis=1),[0,2,1])\n",
    "    # context_query attention\n",
    "    A = tf.matmul(S_, query)\n",
    "    # query_context attention\n",
    "    B = tf.matmul(tf.matmul(S_, S__T), context)\n",
    "    return A, B\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    q_input = tf.placeholder(tf.int32, [None, 30], name=\"q\")\n",
    "    q_char_input = tf.placeholder(tf.int32, [None, 480], name=\"q_char\")\n",
    "    c_input = tf.placeholder(tf.int32, [None, 240], name=\"c\")\n",
    "    c_char_input = tf.placeholder(tf.int32, [None, 3840], name=\"c_char\")\n",
    "    \n",
    "    start_mask = tf.placeholder(tf.int32, [None], name=\"start_mask\")\n",
    "    end_mask = tf.placeholder(tf.int32, [None], name=\"end_mask\")\n",
    "    \n",
    "    batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "    dp = tf.placeholder(tf.float32, (), name=\"drop_prob\")\n",
    "\n",
    "tf.add_to_collection(\"infer_input\", q_input)\n",
    "tf.add_to_collection(\"infer_input\", q_char_input)\n",
    "tf.add_to_collection(\"infer_input\", c_input)\n",
    "tf.add_to_collection(\"infer_input\", c_char_input)\n",
    "tf.add_to_collection(\"infer_input\", dp)\n",
    "\n",
    "with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "    # input embedding layer\n",
    "    with tf.variable_scope(\"W_Embedding\"):\n",
    "        pretrained_embedding = tf.get_variable(\"w_embedding\",\n",
    "                                               shape=[72497,300],\n",
    "                                               initializer=tf.constant_initializer(embedding),\n",
    "                                               trainable=False)\n",
    "        unknown_embedding = tf.get_variable(\"unknown\",\n",
    "                                            shape=[1, 300],\n",
    "                                            initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                            trainable=True)\n",
    "        tf.summary.histogram(\"unknown_word_embedding\", unknown_embedding)\n",
    "        padding_embedding = tf.get_variable(\"padding\",\n",
    "                                            shape=[1, 300],\n",
    "                                            initializer=tf.zeros_initializer(),\n",
    "                                            trainable=False)\n",
    "        word_embedding = tf.concat([pretrained_embedding, unknown_embedding, padding_embedding], 0)\n",
    "        \n",
    "        q_embed = tf.nn.embedding_lookup(word_embedding, q_input)\n",
    "        q_embed = tf.nn.dropout(q_embed, 1-dp)\n",
    "        c_embed = tf.nn.embedding_lookup(word_embedding, c_input)\n",
    "        c_embed = tf.nn.dropout(c_embed, 1-dp)\n",
    "\n",
    "    with tf.variable_scope(\"C_Embedding\"):\n",
    "        char_embedding = tf.get_variable(\"c_embedding\",\n",
    "                                         shape=[209, 200],\n",
    "                                         initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                         trainable=True)\n",
    "        padding = tf.get_variable(\"padding\",\n",
    "                                  shape=[1, 200],\n",
    "                                  initializer=tf.zeros_initializer(),\n",
    "                                  trainable=False)\n",
    "        char_combined = tf.concat([char_embedding, padding], 0)\n",
    "        tf.summary.histogram(\"character_embedding\", char_combined)\n",
    "        q_char_embed = tf.nn.embedding_lookup(char_combined, q_char_input)\n",
    "        c_char_embed = tf.nn.embedding_lookup(char_combined, c_char_input)\n",
    "        \n",
    "        squeeze_to_word_q = tf.layers.max_pooling1d(q_char_embed, 16, 16)\n",
    "        squeeze_to_word_q = tf.nn.dropout(squeeze_to_word_q, 1-dp*0.5)\n",
    "        squeeze_to_word_c = tf.layers.max_pooling1d(c_char_embed, 16, 16)\n",
    "        squeeze_to_word_c = tf.nn.dropout(squeeze_to_word_c, 1-dp*0.5)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_output\"):\n",
    "        q_embed_out = tf.concat([q_embed, squeeze_to_word_q], 2)\n",
    "        c_embed_out = tf.concat([c_embed, squeeze_to_word_c], 2)\n",
    "        q_embed_out = highway(\"highway\", q_embed_out)\n",
    "        c_embed_out = highway(\"highway\", c_embed_out)\n",
    "\n",
    "with tf.variable_scope(\"Embedding_Encoder_Layer\"):\n",
    "    # embedding encoder layer\n",
    "    q_encoded = embedding_encoder_block(\"encoder_block\", q_embed_out)\n",
    "    c_encoded = embedding_encoder_block(\"encoder_block\", c_embed_out)\n",
    "    print(\"Embedding Encoder Layer output shape:\", q_encoded.shape, c_encoded.shape)\n",
    "    \n",
    "with tf.variable_scope(\"Context_Query_Attention_Layer\"):\n",
    "    # context_query attention layer\n",
    "    # first compute similarity matrix between context and query\n",
    "    # S_tj = w * [C_t; Q_j; C_t*Q_j]\n",
    "    c_expand = tf.expand_dims(c_encoded, 2)\n",
    "    c_expand = tf.tile(c_expand, [1,1,30,1])\n",
    "    \n",
    "    q_expand = tf.expand_dims(q_encoded, 1)\n",
    "    q_expand = tf.tile(q_expand, [1,240,1,1])\n",
    "    \n",
    "    qc_mul = tf.multiply(c_expand, q_expand)\n",
    "    \n",
    "    qc_concat = tf.concat([c_expand,q_expand,qc_mul], 3)\n",
    "    w = tf.get_variable(\"s_w\", [384,1])\n",
    "    tf.summary.histogram(\"S_matrix_weight\", w)\n",
    "    \n",
    "    A, B = query_context_co_attention(w, qc_concat, c_encoded, q_encoded)\n",
    "    \n",
    "    # layer output\n",
    "    G = tf.concat([c_encoded, A, tf.multiply(c_encoded,A), tf.multiply(c_encoded,B)],2)\n",
    "    print(\"Co-Attention Layer output shape:\", G.shape)\n",
    "\n",
    "with tf.variable_scope(\"Model_Encoder_Layer\"):\n",
    "    # model encoder layer\n",
    "    model_encoder1 = model_encoder_block(\"model_encoder\", G, projection=True)\n",
    "    model_encoder2 = model_encoder_block(\"model_encoder\", model_encoder1, projection=False)\n",
    "    model_encoder3 = model_encoder_block(\"model_encoder\", model_encoder2, projection=False)\n",
    "    \n",
    "    print(\"Model Encoder Layer output shape:\",model_encoder1.shape,model_encoder2.shape,model_encoder3.shape)\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    # output layer\n",
    "    # p1: start probability sequence\n",
    "    # p2: end probability sequence\n",
    "    p1_input = tf.concat([model_encoder1, model_encoder2],2)\n",
    "    p2_input = tf.concat([model_encoder2, model_encoder3],2)\n",
    "    \n",
    "    p1_logits = tf.squeeze(tf.layers.separable_conv1d(p1_input, 1, 1),-1)\n",
    "    p2_logits = tf.squeeze(tf.layers.separable_conv1d(p2_input, 1, 1),-1)\n",
    "    \n",
    "    p1_prob = tf.nn.softmax(p1_logits)\n",
    "    p2_prob = tf.nn.softmax(p2_logits)\n",
    "    \n",
    "    s_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(start_mask,1)],1)\n",
    "    e_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(end_mask,1)],1)\n",
    "    yhat_p1 = tf.add(tf.gather_nd(p1_prob, s_pairs), 1e-15)\n",
    "    yhat_p2 = tf.add(tf.gather_nd(p2_prob, e_pairs), 1e-15)\n",
    "\n",
    "tf.add_to_collection(\"predictions\", p1_prob)\n",
    "tf.add_to_collection(\"predictions\", p2_prob)\n",
    "\n",
    "global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "    \n",
    "with tf.variable_scope(\"Optimizer\"):\n",
    "    \n",
    "    # add l2 weight decay to all variables\n",
    "    trainables = tf.trainable_variables()\n",
    "    loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in trainables if 'bias' not in v.name ]) * 3e-7\n",
    "    tf.summary.histogram(\"l2_loss\", loss_l2)\n",
    "    loss = -tf.reduce_mean(tf.log(yhat_p1) + tf.log(yhat_p2)) + loss_l2\n",
    "    \n",
    "    # perform cold warm up and gradient clipping\n",
    "    lr = tf.minimum(0.001, 0.001 / tf.log(999.) * tf.log(tf.cast(global_step, tf.float32) + 1))\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=0.8,epsilon=1e-7)\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    opt_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "    \n",
    "    # apply exponential moving average\n",
    "    # used in inference\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "    with tf.control_dependencies([opt_op]):\n",
    "        train_step = ema.apply(trainables)\n",
    "\n",
    "tf.add_to_collection(\"train_step\", train_step)\n",
    "        \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "f_measure_train = tf.get_variable(\"f_train\", (), trainable=False)\n",
    "f_measure_dev = tf.get_variable(\"f_dev\", (), trainable=False)\n",
    "tf.summary.scalar(\"f_train\", f_measure_train)\n",
    "tf.summary.scalar(\"f_dev\", f_measure_dev)\n",
    "print(\"Prob distribution shape:\", p1_prob.shape, p2_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During inference, we load the shadow variables(exponential moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        variables_to_restore = ema.variables_to_restore()\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./model/checkpoint'))\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        gen = dev_batch()\n",
    "        #saver = tf.train.Saver()\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        if train:\n",
    "            sess.run(train_iter.initializer)\n",
    "\n",
    "        s_idx, e_idx = tf.get_collection(\"predictions\")\n",
    "        f_list = []\n",
    "        \n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            \n",
    "            try:\n",
    "                c_d, c_char_d, q_d, q_char_d, ans_d = next(gen)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            feed_dict={q_input:q_d,\n",
    "                         q_char_input: q_char_d,\n",
    "                         c_input: c_d,\n",
    "                         c_char_input: c_char_d,\n",
    "                         dp: 0}\n",
    "            \n",
    "            pred_s, pred_e = sess.run([s_idx, e_idx], feed_dict=feed_dict)\n",
    "            f = f_dev(pred_s, pred_e, ans_d, c_d)\n",
    "            \n",
    "            print(f)\n",
    "            f_list.append(f)\n",
    "\n",
    "print(\"Done!\")\n",
    "print(sum(f_list)/len(f_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
