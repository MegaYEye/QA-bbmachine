{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the shared-normalization method in [Simple and Effective Multi-Paragraph Reading Comprehension](https://arxiv.org/abs/1710.10723). This does not show a better performance over our single paragraph model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = np.load(\"embedding.npy\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"documents.json\") as f:\n",
    "    doc = json.load(f)\n",
    "with open(\"training.json\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"word_dict.json\",\"r\") as f:\n",
    "    word_dict = json.load(f)\n",
    "with open(\"char_dict.json\",\"r\") as f:\n",
    "    char_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topk = 4\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "punc = set(['\"','\\'',\"?\",\".\",\",\",\"/\",\"<\",\">\",\":\",\";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown_detection(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token in word_dict:\n",
    "            new_list.append(token)\n",
    "        else:\n",
    "            new_list.append(\"<UNK>\")\n",
    "    return new_list\n",
    "\n",
    "def generate_char(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token == \"<PAD>\":\n",
    "            char_list = [\"<pad>\"]*16\n",
    "        else:\n",
    "            char_list = [c for c in token[:16]]\n",
    "        while len(char_list) < 16:\n",
    "            char_list.append(\"<pad>\")\n",
    "        for char in char_list:\n",
    "            if char in char_dict:\n",
    "                new_list.append(char)\n",
    "            else:\n",
    "                new_list.append(\"<unk>\")\n",
    "    return new_list\n",
    "\n",
    "if os.path.exists(\"tfidfs.pickle\"):\n",
    "    with open(\"tfidfs.pickle\",\"rb\") as f:\n",
    "        tfidfs = pickle.load(f)\n",
    "else:\n",
    "    tfidfs = dict()\n",
    "    for d in doc:\n",
    "        tfidf = TfidfVectorizer(tokenizer=word_tokenize,\n",
    "                                stop_words='english',\n",
    "                                max_df=0.5,\n",
    "                                smooth_idf=False,\n",
    "                                sublinear_tf=True)\n",
    "        paragraphs = [p.lower() for p in d[\"text\"]]\n",
    "        res = tfidf.fit_transform(paragraphs).toarray()\n",
    "        mapping = tfidf.vocabulary_\n",
    "        tfidfs[d[\"docid\"]] = [res, mapping]\n",
    "    with open(\"tfidfs.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tfidfs, f)\n",
    "\n",
    "padded_train = []\n",
    "for sample in tqdm(train):\n",
    "    new_sample = dict()\n",
    "    docid = sample[\"docid\"]\n",
    "\n",
    "    question = word_tokenize(sample[\"question\"].lower())\n",
    "\n",
    "    answer = word_tokenize(sample[\"text\"].lower())\n",
    "    answer_para = sample[\"answer_paragraph\"]\n",
    "\n",
    "    para = doc[docid][\"text\"][answer_para].lower()\n",
    "    assert(doc[docid][\"docid\"] == docid)\n",
    "    para = word_tokenize(para)[:240]\n",
    "    \n",
    "    # extract indices of answer from paragraph\n",
    "    answer_idx = None\n",
    "    for i, j in enumerate(para):\n",
    "        if j == answer[0]:\n",
    "            k = 1\n",
    "            while k < len(answer) and i+k<len(para):\n",
    "                if para[i+k] != answer[k]:\n",
    "                    break\n",
    "                k += 1\n",
    "            else:\n",
    "                answer_idx = (i, i+k)\n",
    "                break\n",
    "    # ignore samples that no answer can be found\n",
    "    if answer_idx is None:\n",
    "        continue\n",
    "    \n",
    "    # select topk noisy sample by tfidf\n",
    "    rmed = []\n",
    "    for token in question:\n",
    "        if token not in stopword and token not in punc:\n",
    "            rmed.append(token)\n",
    "    \n",
    "    res, mapping = tfidfs[docid]\n",
    "    # set accumulator for each paragraph\n",
    "    a_d = [0 for _ in range(res.shape[0])]\n",
    "    for token in rmed:\n",
    "        for i in range(len(a_d)):\n",
    "            if token in mapping:\n",
    "                a_d[i] += res[i, mapping[token]]\n",
    "            else:\n",
    "                pass\n",
    "    k = topk if res.shape[0] > topk else res.shape[0]\n",
    "    pred = np.argpartition(a_d, -k)[-k:]\n",
    "    pred = set(pred)\n",
    "    # give 3 noisy samples excluding the correct one\n",
    "    if answer_para in pred:\n",
    "        pred.remove(answer_para)\n",
    "    else:\n",
    "        pred.pop()\n",
    "    pred = list(pred)\n",
    "    \n",
    "    while len(para) < 240:\n",
    "        para.append(\"<PAD>\")\n",
    "    content_char = generate_char(para)\n",
    "    content = unknown_detection(para)\n",
    "    \n",
    "    noisy = []\n",
    "    noisy_char = []\n",
    "    for i in range(3):\n",
    "        noise = doc[docid][\"text\"][pred[i]].lower()\n",
    "        noise = word_tokenize(noise)[:240]\n",
    "        while len(noise) < 240:\n",
    "            noise.append(\"<PAD>\")\n",
    "        noise_char = generate_char(noise)\n",
    "        noise = unknown_detection(noise)\n",
    "        noisy.append(noise)\n",
    "        noisy_char.append(noise_char)\n",
    "        assert len(noise) == 240\n",
    "        assert len(noise_char) == 3840\n",
    "    \n",
    "    while len(answer) < 7:\n",
    "        answer.append(\"<PAD>\")\n",
    "    answer = answer[:7]\n",
    "    answer_char = generate_char(answer)\n",
    "    answer = unknown_detection(answer)\n",
    "    \n",
    "    padded_question = question[:30]\n",
    "    while len(padded_question) < 30:\n",
    "        padded_question.append(\"<PAD>\")\n",
    "    question_char = generate_char(padded_question)\n",
    "    padded_question = unknown_detection(padded_question)\n",
    "    \n",
    "    new_sample[\"question\"] = padded_question\n",
    "    new_sample[\"q_char\"] = question_char\n",
    "    new_sample[\"content\"] = content\n",
    "    new_sample[\"c_char\"] = content_char\n",
    "    new_sample[\"answer\"] = answer\n",
    "    new_sample[\"answer_char\"] = answer_char\n",
    "    new_sample[\"answer_idx\"] = answer_idx\n",
    "    for i in range(3):\n",
    "        new_sample[\"noisy\"+str(i+1)] = noisy[i]\n",
    "        new_sample[\"noisy_char\"+str(i+1)] = noisy_char[i]\n",
    "    \n",
    "    assert len(padded_question) == 30\n",
    "    assert len(question_char) == 480\n",
    "    assert len(content) == 240\n",
    "    assert len(content_char) == 3840\n",
    "    assert len(answer) == 7\n",
    "    assert len(answer_char) == 112\n",
    "    assert len(answer_idx) == 2\n",
    "    \n",
    "    padded_train.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    print(padded_train[0][\"noisy\"+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(padded):\n",
    "    \"\"\"\n",
    "    input for NN:\n",
    "        c:           content token\n",
    "        c_char:      content character\n",
    "        q:           question token\n",
    "        q_char:      question character\n",
    "        n:           noisy paragraph\n",
    "        n_char:      noisy paragraph character\n",
    "    output:\n",
    "        probability distribution of start and end position over context\n",
    "    \"\"\"\n",
    "    c, c_chars, q, q_chars, a_idx = [], [], [], [], []\n",
    "    n1, n_char1, n2, n_char2, n3, n_char3 = [], [], [], [], [], []\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in tqdm(range(len(padded))):\n",
    "        cnt += 1\n",
    "        sample = padded[i]\n",
    "        question = sample[\"question\"]\n",
    "        content = sample[\"content\"]\n",
    "        q_char = sample[\"q_char\"]\n",
    "        c_char = sample[\"c_char\"]\n",
    "        aidx = sample[\"answer_idx\"]\n",
    "        answer = sample[\"answer\"]\n",
    "        noisy1 = sample[\"noisy1\"]\n",
    "        noisy_char1 = sample[\"noisy_char1\"]\n",
    "        noisy2 = sample[\"noisy2\"]\n",
    "        noisy_char2 = sample[\"noisy_char2\"]\n",
    "        noisy3 = sample[\"noisy3\"]\n",
    "        noisy_char3 = sample[\"noisy_char3\"]\n",
    "        \n",
    "        q_mapped = [word_dict[t] for t in question]\n",
    "        c_mapped = [word_dict[t] for t in content]\n",
    "        n1_mapped = [word_dict[t] for t in noisy1]\n",
    "        n2_mapped = [word_dict[t] for t in noisy2]\n",
    "        n3_mapped = [word_dict[t] for t in noisy3]\n",
    "        q_char_mapped = [char_dict[ch] for ch in q_char]\n",
    "        c_char_mapped = [char_dict[ch] for ch in c_char]\n",
    "        n_char1_mapped = [char_dict[ch] for ch in noisy_char1]\n",
    "        n_char2_mapped = [char_dict[ch] for ch in noisy_char2]\n",
    "        n_char3_mapped = [char_dict[ch] for ch in noisy_char3]\n",
    "        \n",
    "        c.append(c_mapped)\n",
    "        q.append(q_mapped)\n",
    "        c_chars.append(c_char_mapped)\n",
    "        q_chars.append(q_char_mapped)\n",
    "        a_idx.append(aidx)\n",
    "        n1.append(n1_mapped)\n",
    "        n2.append(n2_mapped)\n",
    "        n3.append(n3_mapped)\n",
    "        n_char1.append(n_char1_mapped)\n",
    "        n_char2.append(n_char2_mapped)\n",
    "        n_char3.append(n_char3_mapped)\n",
    "        \n",
    "        padded[i] = None # clear memory usage\n",
    "        \n",
    "    return np.array(c), np.array(c_chars), np.array(q), np.array(q_chars), np.array(a_idx), np.array(n1), np.array(n_char1), np.array(n2), np.array(n_char2), np.array(n3), np.array(n_char3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, c_char, q, q_char, a_idx, n1, n_char1, n2, n_char2, n3, n_char3 = generate_training_data(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(n3.shape, n_char3.shape)\n",
    "print(c.shape, c_char.shape)\n",
    "print(q.shape, q_char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "filename = \"train2.tfrecords\"\n",
    "with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "    for i in range(len(c)):\n",
    "        example = tf.train.Example(\n",
    "              features=tf.train.Features(\n",
    "                  feature={\n",
    "                      'c': _int64_feature(c[i]),\n",
    "                      'c_char': _int64_feature(c_char[i]),\n",
    "                      'q': _int64_feature(q[i]),\n",
    "                      'q_char': _int64_feature(q_char[i]),\n",
    "                      'a_idx': _int64_feature(a_idx[i]),\n",
    "                      'n1':  _int64_feature(n1[i]),\n",
    "                      'n2':  _int64_feature(n2[i]),\n",
    "                      'n3':  _int64_feature(n3[i]),\n",
    "                      'n_char1':  _int64_feature(n_char1[i]),\n",
    "                      'n_char2':  _int64_feature(n_char2[i]),\n",
    "                      'n_char3':  _int64_feature(n_char3[i])\n",
    "                  }))\n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_score(pred_s, pred_e, pred_sn, pred_en, true_s, true_e, context):\n",
    "    # computes average f_measure for a batch\n",
    "    f_sum = 0\n",
    "    l = len(pred_s)\n",
    "    for i in range(l):\n",
    "        pred_pair, score1 = calcu_score(pred_s[i], pred_e[i])\n",
    "        pred_pair_n, score2 = calcu_score(pred_sn[i], pred_en[i])\n",
    "        if score1 < score2 or pred_pair[1] < pred_pair[0]:\n",
    "            continue\n",
    "        TP, FN, FP = 0, 0, 0\n",
    "        guess = context[i][pred_pair[0]:pred_pair[1]+1]\n",
    "        true = context[i][true_s[i]:true_e[i]+1]\n",
    "        for token in guess:\n",
    "            if token in true:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        for token in true:\n",
    "            if token not in guess:\n",
    "                FN += 1\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        f = 2*precision*recall/(precision+recall+1e-8)\n",
    "        f_sum += f\n",
    "    return f_sum/l\n",
    "\n",
    "def calcu_score(slist, elist):\n",
    "    sidx = np.argmax(slist)\n",
    "    eidx = np.argmax(elist)\n",
    "    score = slist[sidx] + elist[eidx]\n",
    "    return [sidx, eidx], score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "filename = \"train2.tfrecords\"\n",
    "dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"c\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"c_char\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"q\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"q_char\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"a_idx\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n1\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n2\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n3\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n_char1\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n_char2\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "        \"n_char3\": tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "    c = tf.cast(parsed[\"c\"], tf.int32)\n",
    "    c_char = tf.cast(parsed[\"c_char\"], tf.int32)\n",
    "    q = tf.cast(parsed[\"q\"], tf.int32)\n",
    "    q_char = tf.cast(parsed[\"q_char\"], tf.int32)\n",
    "    a_idx = tf.cast(parsed[\"a_idx\"], tf.int32)\n",
    "    n1 = tf.cast(parsed[\"n1\"], tf.int32)\n",
    "    n2 = tf.cast(parsed[\"n2\"], tf.int32)\n",
    "    n3 = tf.cast(parsed[\"n3\"], tf.int32)\n",
    "    n_char1 = tf.cast(parsed[\"n_char1\"], tf.int32)\n",
    "    n_char2 = tf.cast(parsed[\"n_char2\"], tf.int32)\n",
    "    n_char3 = tf.cast(parsed[\"n_char3\"], tf.int32)\n",
    "\n",
    "    return c, c_char, q, q_char, a_idx, n1, n_char1, n2, n_char2, n3, n_char3\n",
    "\n",
    "epoch = 1000\n",
    "batch = 12\n",
    "\n",
    "def make_dataset(dataset):\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, epoch))\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(batch)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n",
    "\n",
    "train_iter = make_dataset(dataset)\n",
    "next_batch = train_iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        #convolution block\n",
    "        pos_encoded = position_encoding(inputs)\n",
    "        residual1 = tf.layers.conv1d(pos_encoded, 128, 1, padding=\"same\",activation=tf.nn.relu)\n",
    "        \n",
    "        norm1 = tf.contrib.layers.layer_norm(residual1)\n",
    "        norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "        conv1 = tf.layers.conv1d(norm1, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "        residual2 = tf.add(residual1, conv1)\n",
    "        \n",
    "        norm2 = tf.contrib.layers.layer_norm(residual2)\n",
    "        norm2 = tf.nn.dropout(norm2, 1-dp)\n",
    "        conv2 = tf.layers.conv1d(norm2, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv2 = tf.nn.dropout(conv2, 1-dp)\n",
    "        residual3 = tf.add(residual2, conv2)\n",
    "        \n",
    "        norm3 = tf.contrib.layers.layer_norm(residual3)\n",
    "        norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "        conv3 = tf.layers.conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv3 = tf.nn.dropout(conv3, 1-dp)\n",
    "        residual4 = tf.add(residual3, conv3)\n",
    "        \n",
    "        norm4 = tf.contrib.layers.layer_norm(residual4)\n",
    "        norm4 = tf.nn.dropout(norm4, 1-dp)\n",
    "        conv4 = tf.layers.conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv4 = tf.nn.dropout(conv4, 1-dp)\n",
    "        residual5 = tf.add(residual4, conv4)\n",
    "        \n",
    "        # self-attention block\n",
    "        norm4 = tf.contrib.layers.layer_norm(residual5)\n",
    "        attention = tf.matmul(norm4, norm4, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(norm4)[-1], dtype=tf.float32)\n",
    "        scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "        attention = tf.nn.softmax(scaled, axis=-1)\n",
    "        attention_out = tf.matmul(attention, norm4)\n",
    "        residual6 = tf.add(residual5, attention_out)\n",
    "        \n",
    "        # feedforwoad layer\n",
    "        norm5 = tf.contrib.layers.layer_norm(residual5)\n",
    "        norm5 = tf.nn.dropout(norm5, 1-dp)\n",
    "        ffn1 = tf.layers.conv1d(norm5, 128, 1, activation=tf.nn.relu)\n",
    "        ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "        ffn2 = tf.layers.conv1d(ffn1, 128, 1)\n",
    "        ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "        residual7 = tf.add(residual6, ffn2)\n",
    "    return residual7\n",
    "\n",
    "def model_encoder_block(scope, inputs, projection=False):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        if projection:\n",
    "            outputs = tf.layers.conv1d(inputs, 128, 1, padding=\"same\", activation=tf.nn.relu)\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        for i in range(7):\n",
    "            with tf.variable_scope(\"conv_block{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm0 = tf.contrib.layers.layer_norm(outputs)\n",
    "                norm0 = tf.nn.dropout(norm0, 1-dp)\n",
    "                conv0 = tf.layers.conv1d(norm0, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv0 = tf.nn.dropout(conv0, 1-dp)\n",
    "                residual0 = tf.add(outputs, conv0)\n",
    "                \n",
    "                norm1 = tf.contrib.layers.layer_norm(residual0)\n",
    "                norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "                conv1 = tf.layers.conv1d(norm1, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "                residual1 = tf.add(residual0, conv1)\n",
    "            \n",
    "            with tf.variable_scope(\"self_attention{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm2 = tf.contrib.layers.layer_norm(residual1)\n",
    "                attention = tf.matmul(norm2, norm2, transpose_b=True)\n",
    "                dk = tf.cast(tf.shape(norm2)[-1], dtype=tf.float32)\n",
    "                scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "                attention = tf.nn.softmax(scaled, axis=-1)\n",
    "                attention_out = tf.matmul(attention, norm2)\n",
    "                residual2 = tf.add(residual1, attention_out)\n",
    "            \n",
    "            with tf.variable_scope(\"feedforward{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm3 = tf.contrib.layers.layer_norm(residual2)\n",
    "                norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "                ffn1 = tf.layers.conv1d(norm3, 128, 1, activation=tf.nn.relu)\n",
    "                ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "                ffn2 = tf.layers.conv1d(ffn1, 128, 1)\n",
    "                ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "                outputs = tf.add(residual2, ffn2)\n",
    "    return outputs\n",
    "\n",
    "def highway(scope, inputs):\n",
    "    # two layer highway network\n",
    "    size = inputs.shape.as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        T1 = tf.layers.conv1d(inputs, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H1 = tf.layers.conv1d(inputs, size, 1)\n",
    "        H1 = tf.nn.dropout(H1, 1-dp)\n",
    "        highway1 = T1 * H1 + inputs * (1.0 - T1)\n",
    "        \n",
    "        T2 = tf.layers.conv1d(highway1, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H2 = tf.layers.conv1d(highway1, size, 1)\n",
    "        H2 = tf.nn.dropout(H2, 1-dp)\n",
    "        highway2 = T2 * H2 + highway1 * (1.0 - T2)\n",
    "    return highway2\n",
    "\n",
    "def position_encoding(inputs):\n",
    "    \"\"\"\n",
    "    sinusoids position encoding\n",
    "    from Attention Is All You Need\n",
    "    -- input: [None, sequence_length, depth]\n",
    "    -- output: [None, sequence_length, depth]\n",
    "    \"\"\"\n",
    "    _, seq_length, depth = inputs.get_shape().as_list()\n",
    "    pos_encoding = np.array([\n",
    "        [pos / np.power(10000, 2*i/depth) for i in range(depth)]\n",
    "        for pos in range(seq_length)])\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "    pos_encoding = tf.convert_to_tensor(pos_encoding, tf.float32)\n",
    "    return inputs+pos_encoding\n",
    "\n",
    "def query_context_co_attention(w, inputs, context, query):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        w: similarity funciton weight\n",
    "        inputs: [q, c, q*c]\n",
    "    output:\n",
    "        A: context-to-query attention\n",
    "        B: query-to-context attention\n",
    "    \"\"\"\n",
    "    # similarity matrix S (logits)\n",
    "    S = tf.einsum(\"abcde,ef->abcdf\", tf.expand_dims(inputs,3),w)\n",
    "    S = tf.squeeze(S,[-2,-1])\n",
    "    # S_: softmax over rows\n",
    "    S_ = tf.nn.softmax(S)\n",
    "    # S__T: transpose of softmax over coloum\n",
    "    S__T = tf.transpose(tf.nn.softmax(S, axis=1),[0,2,1])\n",
    "    # context_query attention\n",
    "    A = tf.matmul(S_, query)\n",
    "    # query_context attention\n",
    "    B = tf.matmul(tf.matmul(S_, S__T), context)\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    q_input = tf.placeholder(tf.int32, [None, 30], name=\"q\")\n",
    "    q_char_input = tf.placeholder(tf.int32, [None, 480], name=\"q_char\")\n",
    "    c_input = tf.placeholder(tf.int32, [None, 240], name=\"c\")\n",
    "    c_char_input = tf.placeholder(tf.int32, [None, 3840], name=\"c_char\")\n",
    "    n_input = tf.placeholder(tf.int32, [None, 240], name=\"n\")\n",
    "    n_char_input = tf.placeholder(tf.int32, [None, 3840], name=\"n_char\")\n",
    "    \n",
    "    start_mask = tf.placeholder(tf.int32, [None], name=\"start_mask\")\n",
    "    end_mask = tf.placeholder(tf.int32, [None], name=\"end_mask\")\n",
    "    \n",
    "    batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "    dp = tf.placeholder(tf.float32, (), name=\"drop_prob\")\n",
    "\n",
    "tf.add_to_collection(\"infer_input\", q_input)\n",
    "tf.add_to_collection(\"infer_input\", q_char_input)\n",
    "tf.add_to_collection(\"infer_input\", c_input)\n",
    "tf.add_to_collection(\"infer_input\", c_char_input)\n",
    "tf.add_to_collection(\"infer_input\", dp)\n",
    "\n",
    "with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "    # input embedding layer\n",
    "    with tf.variable_scope(\"W_Embedding\"):\n",
    "        pretrained_embedding = tf.get_variable(\"w_embedding\",\n",
    "                                               shape=[72497,300],\n",
    "                                               initializer=tf.constant_initializer(embedding),\n",
    "                                               trainable=False)\n",
    "        unknown_embedding = tf.get_variable(\"unknown\",\n",
    "                                            shape=[1, 300],\n",
    "                                            initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                            trainable=True)\n",
    "        tf.summary.histogram(\"unknown_word_embedding\", unknown_embedding)\n",
    "        padding_embedding = tf.get_variable(\"padding\",\n",
    "                                            shape=[1, 300],\n",
    "                                            initializer=tf.zeros_initializer(),\n",
    "                                            trainable=False)\n",
    "        word_embedding = tf.concat([pretrained_embedding, unknown_embedding, padding_embedding], 0)\n",
    "        \n",
    "        q_embed = tf.nn.embedding_lookup(word_embedding, q_input)\n",
    "        q_embed = tf.nn.dropout(q_embed, 1-dp)\n",
    "        c_embed = tf.nn.embedding_lookup(word_embedding, c_input)\n",
    "        c_embed = tf.nn.dropout(c_embed, 1-dp)\n",
    "        n_embed = tf.nn.embedding_lookup(word_embedding, n_input)\n",
    "        n_embed = tf.nn.dropout(n_embed, 1-dp)\n",
    "\n",
    "    with tf.variable_scope(\"C_Embedding\"):\n",
    "        char_embedding = tf.get_variable(\"c_embedding\",\n",
    "                                         shape=[209, 150],\n",
    "                                         initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                         trainable=True)\n",
    "        padding = tf.get_variable(\"padding\",\n",
    "                                  shape=[1, 150],\n",
    "                                  initializer=tf.zeros_initializer(),\n",
    "                                  trainable=False)\n",
    "        char_combined = tf.concat([char_embedding, padding], 0)\n",
    "        tf.summary.histogram(\"character_embedding\", char_combined)\n",
    "        q_char_embed = tf.nn.embedding_lookup(char_combined, q_char_input)\n",
    "        c_char_embed = tf.nn.embedding_lookup(char_combined, c_char_input)\n",
    "        n_char_embed = tf.nn.embedding_lookup(char_combined, n_char_input)\n",
    "        \n",
    "        squeeze_to_word_q = tf.layers.max_pooling1d(q_char_embed, 16, 16)\n",
    "        squeeze_to_word_q = tf.nn.dropout(squeeze_to_word_q, 1-dp*0.5)\n",
    "        squeeze_to_word_c = tf.layers.max_pooling1d(c_char_embed, 16, 16)\n",
    "        squeeze_to_word_c = tf.nn.dropout(squeeze_to_word_c, 1-dp*0.5)\n",
    "        squeeze_to_word_n = tf.layers.max_pooling1d(n_char_embed, 16, 16)\n",
    "        squeeze_to_word_n = tf.nn.dropout(squeeze_to_word_n, 1-dp*0.5)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_output\"):\n",
    "        q_embed_out = tf.concat([q_embed, squeeze_to_word_q], 2)\n",
    "        c_embed_out = tf.concat([c_embed, squeeze_to_word_c], 2)\n",
    "        n_embed_out = tf.concat([n_embed, squeeze_to_word_n], 2)\n",
    "        q_embed_out = highway(\"highway\", q_embed_out)\n",
    "        c_embed_out = highway(\"highway\", c_embed_out)\n",
    "        n_embed_out = highway(\"highway\", n_embed_out)\n",
    "\n",
    "with tf.variable_scope(\"Embedding_Encoder_Layer\"):\n",
    "    # embedding encoder layer\n",
    "    q_encoded = embedding_encoder_block(\"encoder_block\", q_embed_out)\n",
    "    c_encoded = embedding_encoder_block(\"encoder_block\", c_embed_out)\n",
    "    n_encoded = embedding_encoder_block(\"encoder_block\", n_embed_out)\n",
    "    print(q_encoded.shape, c_encoded.shape, n_encoded.shape)\n",
    "    \n",
    "with tf.variable_scope(\"Context_Query_Attention_Layer\"):\n",
    "    # context_query attention layer\n",
    "    # first compute similarity matrix between context and query\n",
    "    # S_tj = w * [C_t; Q_j; C_t*Q_j]\n",
    "    c_expand = tf.expand_dims(c_encoded, 2)\n",
    "    c_expand = tf.tile(c_expand, [1,1,30,1])\n",
    "    \n",
    "    n_expand = tf.expand_dims(n_encoded, 2)\n",
    "    n_expand = tf.tile(n_expand, [1,1,30,1])\n",
    "    \n",
    "    q_expand = tf.expand_dims(q_encoded, 1)\n",
    "    q_expand = tf.tile(q_expand, [1,240,1,1])\n",
    "    \n",
    "    qc_mul = tf.multiply(c_expand, q_expand)\n",
    "    qn_mul = tf.multiply(n_expand, q_expand)\n",
    "    \n",
    "    qc_concat = tf.concat([c_expand,q_expand,qc_mul], 3)\n",
    "    qn_concat = tf.concat([n_expand,q_expand,qn_mul], 3)\n",
    "    w = tf.get_variable(\"s_w\", [384,1])\n",
    "    tf.summary.histogram(\"S_matrix_weight\", w)\n",
    "    \n",
    "    A1, B1 = query_context_co_attention(w, qc_concat, c_encoded, q_encoded)\n",
    "    A2, B2 = query_context_co_attention(w, qn_concat, n_encoded, q_encoded)\n",
    "    \n",
    "    # layer output\n",
    "    G_c = tf.concat([c_encoded, A1, tf.multiply(c_encoded,A1), tf.multiply(c_encoded,B1)],2)\n",
    "    G_n = tf.concat([n_encoded, A2, tf.multiply(n_encoded,A2), tf.multiply(n_encoded,B2)],2)\n",
    "    print(G_c.shape, G_n.shape)\n",
    "\n",
    "with tf.variable_scope(\"Model_Encoder_Layer\"):\n",
    "    # model encoder layer\n",
    "    model_encoder_c1 = model_encoder_block(\"model_encoder\", G_c, projection=True)\n",
    "    model_encoder_c2 = model_encoder_block(\"model_encoder\", model_encoder_c1, projection=False)\n",
    "    model_encoder_c3 = model_encoder_block(\"model_encoder\", model_encoder_c2, projection=False)\n",
    "    \n",
    "    model_encoder_n1 = model_encoder_block(\"model_encoder\", G_n, projection=True)\n",
    "    model_encoder_n2 = model_encoder_block(\"model_encoder\", model_encoder_n1, projection=False)\n",
    "    model_encoder_n3 = model_encoder_block(\"model_encoder\", model_encoder_n2, projection=False)\n",
    "    \n",
    "    print(model_encoder_c1.shape,model_encoder_c2.shape,model_encoder_c3.shape)\n",
    "    print(model_encoder_n1.shape,model_encoder_n2.shape,model_encoder_n3.shape)\n",
    "\n",
    "global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    # output layer\n",
    "    # p1: start probability sequence\n",
    "    # p2: end probability sequence\n",
    "    p1_input_c = tf.concat([model_encoder_c1, model_encoder_c2],2)\n",
    "    p2_input_c = tf.concat([model_encoder_c2, model_encoder_c3],2)\n",
    "    p1_input_n = tf.concat([model_encoder_n1, model_encoder_n2],2)\n",
    "    p2_input_n = tf.concat([model_encoder_n2, model_encoder_n3],2)\n",
    "    \n",
    "    p1_logits_c = tf.squeeze(tf.layers.conv1d(p1_input_c, 1, 1),-1)\n",
    "    p2_logits_c = tf.squeeze(tf.layers.conv1d(p2_input_c, 1, 1),-1)\n",
    "    p1_logits_n = tf.squeeze(tf.layers.conv1d(p1_input_n, 1, 1),-1)\n",
    "    p2_logits_n = tf.squeeze(tf.layers.conv1d(p2_input_n, 1, 1),-1)\n",
    "    \n",
    "    p1_prob = tf.nn.softmax(tf.concat([p1_logits_c, p1_logits_n], 1))\n",
    "    p2_prob = tf.nn.softmax(tf.concat([p2_logits_c, p2_logits_n], 1))\n",
    "    \n",
    "    s_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(start_mask,1)],1)\n",
    "    e_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(end_mask,1)],1)\n",
    "    yhat_p1 = tf.add(tf.gather_nd(p1_prob, s_pairs), 1e-15)\n",
    "    yhat_p2 = tf.add(tf.gather_nd(p2_prob, e_pairs), 1e-15)\n",
    "\n",
    "tf.add_to_collection(\"predictions\", p1_logits_c)\n",
    "tf.add_to_collection(\"predictions\", p2_logits_c)\n",
    "    \n",
    "with tf.variable_scope(\"Optimizer\"):\n",
    "    # add l2 weight decay to all variables\n",
    "    trainables = tf.trainable_variables()\n",
    "    loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in trainables if 'bias' not in v.name ]) * 3e-7\n",
    "    tf.summary.histogram(\"l2_loss\", loss_l2)\n",
    "    loss = -tf.reduce_mean(tf.log(yhat_p1) + tf.log(yhat_p2)) + loss_l2\n",
    "    \n",
    "    # perform cold warm up and gradient clipping\n",
    "    lr = tf.minimum(0.001, 0.001 / tf.log(999.) * tf.log(tf.cast(global_step, tf.float32) + 1))\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=0.8,epsilon=1e-7)\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    opt_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "    \n",
    "    # apply exponential moving average\n",
    "    # used in inference\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "    with tf.control_dependencies([opt_op]):\n",
    "        train_step = ema.apply(trainables)\n",
    "\n",
    "tf.add_to_collection(\"train_step\", train_step)\n",
    "        \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "f_measure = tf.get_variable(\"f_measure\", (), trainable=False)\n",
    "tf.summary.scalar(\"f_measure\", f_measure)\n",
    "print(p1_prob.shape, p2_prob.shape)\n",
    "print(yhat_p1.shape, yhat_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(train_iter.initializer)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=3)\n",
    "        \n",
    "        cnt = 0\n",
    "        f = 0\n",
    "        while True:\n",
    "            try:\n",
    "                cnt += 1\n",
    "                next_c, next_c_char, next_q, next_q_char, next_mask, next_n1, next_n_char1, next_n2, next_n_char2, next_n3, next_n_char3 = sess.run(next_batch)\n",
    "                \n",
    "                next_smask = next_mask[:,0]\n",
    "                next_emask = next_mask[:,1]-1\n",
    "                # randomly sample a noisy paragraph\n",
    "                seed = random.randint(1,3)\n",
    "                if seed == 1:\n",
    "                    next_n, next_n_char = next_n1, next_n_char1\n",
    "                elif seed == 2:\n",
    "                    next_n, next_n_char = next_n2, next_n_char2\n",
    "                else:\n",
    "                    next_n, next_n_char = next_n3, next_n_char3\n",
    "                    \n",
    "                feed_dict = {q_input: next_q,\n",
    "                             q_char_input: next_q_char,\n",
    "                             c_input: next_c,\n",
    "                             c_char_input: next_c_char,\n",
    "                             n_input: next_n,\n",
    "                             n_char_input: next_n_char,\n",
    "                             start_mask: next_smask,\n",
    "                             end_mask: next_emask,\n",
    "                             f_measure:f,\n",
    "                             batch_size: len(next_c),\n",
    "                             dp:0.1}\n",
    "                \n",
    "                run_ops = [train_step, p1_logits_c, p2_logits_c, p1_logits_n, p2_logits_n, global_step, merged]\n",
    "                \n",
    "                if cnt % 100 == 99:\n",
    "                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    _, ps, pe, psn, pen, step, s = sess.run(run_ops, feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "                    writer.add_run_metadata(run_metadata, \"steps{}\".format(step), global_step=step)\n",
    "                    writer.add_summary(s, step)\n",
    "                else:\n",
    "                    _, ps, pe, psn, pen, step, s = sess.run(run_ops, feed_dict=feed_dict)\n",
    "                    writer.add_summary(s, step)\n",
    "                \n",
    "                if cnt % 50 == 0:\n",
    "                    f = f_score(ps, pe, psn, pen, next_smask, next_emask, next_c)\n",
    "                \n",
    "                if cnt % 1500 == 0:\n",
    "                    print(cnt)\n",
    "                    saver.save(sess, \"model/strong\", global_step=step)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                saver.save(sess, \"model/strong\", global_step=step)\n",
    "        print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
