{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linrong/QA/v100-run2\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "%cd \"/home/linrong/QA/v100-run2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/linrong/floyd/qa/documents.json\") as f:\n",
    "    doc = json.load(f)\n",
    "with open(\"/home/linrong/Downloads/project_files/devel.json\") as f:\n",
    "    dev = json.load(f)\n",
    "with open(\"/home/linrong/floyd/qa/mapping.json\") as f:\n",
    "    word_dict = json.load(f)\n",
    "with open(\"/home/linrong/floyd/qa/char_dict.json\") as f:\n",
    "    char_dict = json.load(f)\n",
    "embedding = np.load(\"/home/linrong/floyd/qa/embedding.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/linrong/floyd/qa/training.json\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_p = 3\n",
    "topk_s = 3\n",
    "stopword = set(stopwords.words('english'))\n",
    "punc = set(['\"','\\'',\"?\",\".\",\",\",\"/\",\"<\",\">\",\":\",\";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/3097 [00:00<00:21, 141.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrices loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3097/3097 [00:20<00:00, 153.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def unknown_detection(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token in word_dict:\n",
    "            new_list.append(token)\n",
    "        else:\n",
    "            new_list.append(\"<UNK>\")\n",
    "    return new_list\n",
    "\n",
    "def unknown_detection_char(char_list):\n",
    "    for i in range(len(char_list)):\n",
    "        if char_list[i] not in char_dict:\n",
    "            char_list[i] = \"<unk>\"\n",
    "    return char_list\n",
    "\n",
    "def generate_char(token_list):\n",
    "    new_list = []\n",
    "    for token in token_list:\n",
    "        if token == \"<PAD>\":\n",
    "            char_list = [\"<pad>\"]*16\n",
    "        else:\n",
    "            char_list = [c for c in token[:16]]\n",
    "        while len(char_list) < 16:\n",
    "            char_list.append(\"<pad>\")\n",
    "        for char in char_list:\n",
    "            if char in char_dict:\n",
    "                new_list.append(char)\n",
    "            else:\n",
    "                new_list.append(\"<unk>\")\n",
    "    return new_list\n",
    "\n",
    "if os.path.exists(\"tfidfs.pickle\"):\n",
    "    with open(\"tfidfs.pickle\",\"rb\") as f:\n",
    "        tfidfs = pickle.load(f)\n",
    "    tqdm.write(\"matrices loaded\")\n",
    "else:\n",
    "    tfidfs = dict()\n",
    "    for d in doc:\n",
    "        tfidf = TfidfVectorizer(tokenizer=word_tokenize,\n",
    "                                stop_words='english',\n",
    "                                max_df=0.5,\n",
    "                                smooth_idf=False,\n",
    "                                sublinear_tf=True)\n",
    "        paragraphs = [p.lower() for p in d[\"text\"]]\n",
    "        res = tfidf.fit_transform(paragraphs).toarray()\n",
    "        mapping = tfidf.vocabulary_\n",
    "        tfidfs[d[\"docid\"]] = [res, mapping]\n",
    "    with open(\"tfidfs.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tfidfs, f)\n",
    "    tqdm.write(\"matrices building complete\")\n",
    "\n",
    "reproduce = False\n",
    "c = 0\n",
    "padded = []\n",
    "for sample in tqdm(dev):\n",
    "    new_sample = dict()\n",
    "    \n",
    "    docid = sample[\"docid\"]\n",
    "    answer = word_tokenize(sample[\"text\"])[:7]\n",
    "\n",
    "    question = word_tokenize(sample[\"question\"].lower().strip())\n",
    "    rmed = []\n",
    "    for token in question:\n",
    "        if token not in stopword and token not in punc:\n",
    "            rmed.append(token)\n",
    "    question = rmed\n",
    "    \n",
    "    if not reproduce:\n",
    "        res, mapping = tfidfs[docid]\n",
    "        # set accumulator for each paragraph\n",
    "        a_d = [0 for _ in range(res.shape[0])]\n",
    "        for token in question:\n",
    "            for i in range(len(a_d)):\n",
    "                if token in mapping:\n",
    "                    a_d[i] += res[i, mapping[token]]\n",
    "\n",
    "        k = topk_p if res.shape[0] > topk_p else res.shape[0]\n",
    "        pred = np.argpartition(a_d, -k)[-k:]\n",
    "        pred = set(pred)\n",
    "        combined = []\n",
    "        for idx in pred:\n",
    "            sents = sent_tokenize(doc[docid][\"text\"][idx].lower())\n",
    "            for s in sents:\n",
    "                combined.append(s)\n",
    "\n",
    "        # rank sentences in combined sents\n",
    "        tfidf = TfidfVectorizer(smooth_idf=False,\n",
    "                                sublinear_tf=True,\n",
    "                                tokenizer=word_tokenize)\n",
    "        array = tfidf.fit_transform(combined).toarray()\n",
    "        mapping = tfidf.vocabulary_\n",
    "\n",
    "        a_d = np.zeros(len(combined))\n",
    "        for token in question:\n",
    "            for i in range(len(a_d)):\n",
    "                if token in mapping:\n",
    "                    a_d[i] += array[i, mapping[token]]\n",
    "        # return top k results\n",
    "        k = topk_s if len(combined) > topk_s else len(combined)\n",
    "        pred = np.argpartition(a_d, -k)[-k:]\n",
    "        pred = list(OrderedDict.fromkeys(pred))\n",
    "\n",
    "        para = []\n",
    "        for idx in pred:\n",
    "            sent = word_tokenize(combined[idx])[:80]\n",
    "            para += sent\n",
    "    else:\n",
    "        para = word_tokenize(doc[docid][\"text\"][sample[\"answer_paragraph\"]].lower())[:240]\n",
    "        # extract indices of answer from paragraph\n",
    "        answer_idx = None\n",
    "        for i, j in enumerate(para):\n",
    "            if j == answer[0]:\n",
    "                k = 1\n",
    "                while k < len(answer) and i+k<len(para):\n",
    "                    if para[i+k] != answer[k]:\n",
    "                        break\n",
    "                    k += 1\n",
    "                else:\n",
    "                    answer_idx = (i, i+k)\n",
    "                    break\n",
    "        # ignore samples that no answer can be found\n",
    "        if answer_idx is None:\n",
    "            continue\n",
    "    \n",
    "    while len(para) < 240:\n",
    "        para.append(\"<PAD>\")\n",
    "    content_char = generate_char(para)\n",
    "    content = unknown_detection(para)\n",
    "        \n",
    "    padded_question = word_tokenize(sample[\"question\"].lower())[:30]\n",
    "    while len(padded_question) < 30:\n",
    "        padded_question.append(\"<PAD>\")\n",
    "    question_char = generate_char(padded_question)\n",
    "    padded_question = unknown_detection(padded_question)\n",
    "\n",
    "    \n",
    "    new_sample[\"question\"] = padded_question\n",
    "    new_sample[\"q_char\"] = question_char\n",
    "    new_sample[\"content\"] = content\n",
    "    new_sample[\"c_char\"] = content_char\n",
    "    new_sample[\"answer\"] = answer\n",
    "    \n",
    "    assert len(padded_question) == 30\n",
    "    assert len(question_char) == 480\n",
    "    assert len(content) == 240\n",
    "    assert len(content_char) == 3840\n",
    "    assert len(answer) <= 7\n",
    "    \n",
    "    padded.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data(padded):\n",
    "\n",
    "    c, c_chars, q, q_chars, answer = [], [], [], [], []\n",
    "\n",
    "    for sample in tqdm(padded):\n",
    "        question = sample[\"question\"]\n",
    "        content = sample[\"content\"]\n",
    "        q_char = sample[\"q_char\"]\n",
    "        c_char = sample[\"c_char\"]\n",
    "        a = sample[\"answer\"]\n",
    "\n",
    "        q_mapped = [word_dict[t] for t in question]\n",
    "        c_mapped = [word_dict[t] for t in content]\n",
    "        q_char_mapped = [char_dict[ch] for ch in q_char]\n",
    "        c_char_mapped = [char_dict[ch] for ch in c_char]\n",
    "        \n",
    "        c.append(c_mapped)\n",
    "        q.append(q_mapped)\n",
    "        c_chars.append(c_char_mapped)\n",
    "        q_chars.append(q_char_mapped)\n",
    "        answer.append(a)\n",
    "        \n",
    "    return np.array(c), np.array(c_chars), np.array(q), np.array(q_chars), answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3097/3097 [00:00<00:00, 3908.44it/s]\n"
     ]
    }
   ],
   "source": [
    "c, c_char, q, q_char, answer = generate_input_data(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '40', 'billion']\n"
     ]
    }
   ],
   "source": [
    "curr = padded[2550]\n",
    "print(curr[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(pred_s, pred_e, a, context):\n",
    "    # computes average f_measure for a batch\n",
    "    if pred_e < pred_s:\n",
    "        return 0\n",
    "    TP, FN, FP = 0, 0, 0\n",
    "    a = unknown_detection(a)\n",
    "    guess = context[0][pred_s:pred_e+1]\n",
    "    true = [word_dict[x] for x in a]\n",
    "    for token in guess:\n",
    "        if token in true:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    for token in true:\n",
    "        if token not in guess:\n",
    "            FN += 1\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f = 2*precision*recall/(precision+recall+1e-8)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_dp(set1,set2):\n",
    "    assert len(set1) == len(set2)\n",
    "    max1 = 0\n",
    "    maxi1 = 0\n",
    "    maxpair = None\n",
    "    maxp = 0\n",
    "    for i in range(len(set1)):\n",
    "        if set1[i]>max1:\n",
    "            max1 = set1[i]\n",
    "            maxi1 = i\n",
    "        if max1 * set2[i] > maxp:\n",
    "            maxp = max1 * set2[i]\n",
    "            maxpair = [maxi1,i]\n",
    "    assert maxpair[0] <= maxpair[1]\n",
    "    return maxpair,maxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/naive-39000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3097/3097 [04:35<00:00, 11.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "shadow_var = True\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./model/checkpoint'))\n",
    "        #saver = tf.train.import_meta_graph(\"naive-6268.meta\", clear_devices=True)\n",
    "        if shadow_var:\n",
    "            var_to_restore = ema.variables_to_restore()\n",
    "            saver = tf.train.Saver(var_to_restore)\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        c_ph,c_char_ph, q_ph, q_char_ph, dp = tf.get_collection(\"infer_input\")\n",
    "        s_idx, e_idx = tf.get_collection(\"predictions\")\n",
    "        f_dp = []\n",
    "        f_max = []\n",
    "        \n",
    "        for i in tqdm(range(len(c))):\n",
    "            c_in = c[i:i+1,:]\n",
    "            c_char_in = c_char[i:i+1, :]\n",
    "            q_in = q[i:i+1, :]\n",
    "            q_char_in = q_char[i:i+1, :]\n",
    "            a_in = answer[i]\n",
    "            \n",
    "            feed_dict={\"inputs/c:0\": c_in,\n",
    "                       \"inputs/c_char:0\": c_char_in,\n",
    "                       \"inputs/q:0\": q_in,\n",
    "                       \"inputs/q_char:0\": q_char_in,\n",
    "                       \"inputs/drop_prob:0\": 0}\n",
    "            \n",
    "            pred_s, pred_e = sess.run([s_idx, e_idx], feed_dict=feed_dict)\n",
    "            ls = pred_s.tolist()[0]\n",
    "            le = pred_e.tolist()[0]\n",
    "            pair, _ = prob_dp(ls, le)\n",
    "            f1 = f_score(*pair, a_in, c_in)\n",
    "            f2 = f_score(np.argmax(ls), np.argmax(le), a_in, c_in)\n",
    "            f_dp.append(f1)\n",
    "            f_max.append(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5118520292505654\n",
      "0.5016366903479255\n"
     ]
    }
   ],
   "source": [
    "print(sum(f_dp)/len(f_dp))\n",
    "print(sum(f_max)/len(f_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 4], 30)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dp([1,1,6,1,1],[7,2,1,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30, 128) (?, 240, 128)\n",
      "(?, 240, 512)\n",
      "(?, 240, 128) (?, 240, 128) (?, 240, 128)\n",
      "(?,) (?,)\n"
     ]
    }
   ],
   "source": [
    "# todo: depthwise separable convolutions\n",
    "# todo: position encoding\n",
    "# todo: multihead attention(maybe)\n",
    "# todo: regularization(dropout)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def embedding_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        #convolution block\n",
    "        residual1 = tf.layers.separable_conv1d(inputs, 128, 1,activation=tf.nn.relu)\n",
    "        \n",
    "        norm1 = tf.contrib.layers.layer_norm(residual1)\n",
    "        norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "        conv1 = tf.layers.separable_conv1d(norm1, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "        residual2 = tf.add(residual1, conv1)\n",
    "        \n",
    "        norm2 = tf.contrib.layers.layer_norm(residual2)\n",
    "        norm2 = tf.nn.dropout(norm2, 1-dp)\n",
    "        conv2 = tf.layers.separable_conv1d(norm2, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv2 = tf.nn.dropout(conv2, 1-dp)\n",
    "        residual3 = tf.add(residual2, conv2)\n",
    "        \n",
    "        norm3 = tf.contrib.layers.layer_norm(residual3)\n",
    "        norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "        conv3 = tf.layers.separable_conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv3 = tf.nn.dropout(conv3, 1-dp)\n",
    "        residual4 = tf.add(residual3, conv3)\n",
    "        \n",
    "        norm4 = tf.contrib.layers.layer_norm(residual4)\n",
    "        norm4 = tf.nn.dropout(norm4, 1-dp)\n",
    "        conv4 = tf.layers.separable_conv1d(norm3, 128, 7, padding=\"same\",activation=tf.nn.relu)\n",
    "        conv4 = tf.nn.dropout(conv4, 1-dp)\n",
    "        residual5 = tf.add(residual4, conv4)\n",
    "        \n",
    "        # self-attention block\n",
    "        norm4 = tf.contrib.layers.layer_norm(residual5)\n",
    "        attention = tf.matmul(norm4, norm4, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(norm4)[-1], dtype=tf.float32)\n",
    "        scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "        attention = tf.nn.softmax(scaled, axis=-1)\n",
    "        attention_out = tf.matmul(attention, norm4)\n",
    "        residual6 = tf.add(residual5, attention_out)\n",
    "        \n",
    "        # feedforwoad layer\n",
    "        norm5 = tf.contrib.layers.layer_norm(residual5)\n",
    "        norm5 = tf.nn.dropout(norm5, 1-dp)\n",
    "        ffn1 = tf.layers.separable_conv1d(norm5, 128, 1, activation=tf.nn.relu)\n",
    "        ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "        ffn2 = tf.layers.separable_conv1d(ffn1, 128, 1)\n",
    "        ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "        residual7 = tf.add(residual6, ffn2)\n",
    "    return residual7\n",
    "\n",
    "def model_encoder_block(scope, inputs):\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        outputs = inputs\n",
    "        for i in range(7):\n",
    "            with tf.variable_scope(\"conv_block{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm0 = tf.contrib.layers.layer_norm(outputs)\n",
    "                norm0 = tf.nn.dropout(norm0, 1-dp)\n",
    "                conv0 = tf.layers.separable_conv1d(norm0, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv0 = tf.nn.dropout(conv0, 1-dp)\n",
    "                residual0 = tf.add(outputs, conv0)\n",
    "                \n",
    "                norm1 = tf.contrib.layers.layer_norm(residual0)\n",
    "                norm1 = tf.nn.dropout(norm1, 1-dp)\n",
    "                conv1 = tf.layers.separable_conv1d(norm1, 128, 5, padding=\"same\", activation=tf.nn.relu)\n",
    "                conv1 = tf.nn.dropout(conv1, 1-dp)\n",
    "                residual1 = tf.add(residual0, conv1)\n",
    "            \n",
    "            with tf.variable_scope(\"self_attention{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm2 = tf.contrib.layers.layer_norm(residual1)\n",
    "                attention = tf.matmul(norm2, norm2, transpose_b=True)\n",
    "                dk = tf.cast(tf.shape(norm2)[-1], dtype=tf.float32)\n",
    "                scaled = tf.divide(attention, tf.sqrt(dk))\n",
    "                attention = tf.nn.softmax(scaled, axis=-1)\n",
    "                attention_out = tf.matmul(attention, norm2)\n",
    "                residual2 = tf.add(residual1, attention_out)\n",
    "            \n",
    "            with tf.variable_scope(\"feedforward{}\".format(i),reuse=tf.AUTO_REUSE):\n",
    "                norm3 = tf.contrib.layers.layer_norm(residual2)\n",
    "                norm3 = tf.nn.dropout(norm3, 1-dp)\n",
    "                ffn1 = tf.layers.separable_conv1d(norm3, 128, 1, activation=tf.nn.relu)\n",
    "                ffn1 = tf.nn.dropout(ffn1, 1-dp)\n",
    "                ffn2 = tf.layers.separable_conv1d(ffn1, 128, 1)\n",
    "                ffn2 = tf.nn.dropout(ffn2, 1-dp)\n",
    "                outputs = tf.add(residual2, ffn2)\n",
    "    return outputs\n",
    "\n",
    "def highway(scope, inputs):\n",
    "    # two layer highway network\n",
    "    size = inputs.shape.as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        T1 = tf.layers.separable_conv1d(inputs, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H1 = tf.layers.separable_conv1d(inputs, size, 1)\n",
    "        H1 = tf.nn.dropout(H1, 1-dp)\n",
    "        highway1 = T1 * H1 + inputs * (1.0 - T1)\n",
    "        \n",
    "        T2 = tf.layers.separable_conv1d(highway1, size, 1, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1))\n",
    "        H2 = tf.layers.separable_conv1d(highway1, size, 1)\n",
    "        H2 = tf.nn.dropout(H2, 1-dp)\n",
    "        highway2 = T2 * H2 + highway1 * (1.0 - T2)\n",
    "    return highway2\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    q_input = tf.placeholder(tf.int32, [None, 30], name=\"q\")\n",
    "    q_char_input = tf.placeholder(tf.int32, [None, 480], name=\"q_char\")\n",
    "    c_input = tf.placeholder(tf.int32, [None, 240], name=\"c\")\n",
    "    c_char_input = tf.placeholder(tf.int32, [None, 3840], name=\"c_char\")\n",
    "    start_mask = tf.placeholder(tf.int32, [None], name=\"start_mask\")\n",
    "    end_mask = tf.placeholder(tf.int32, [None], name=\"end_mask\")\n",
    "    batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "    dp = tf.placeholder(tf.float32, (), name=\"drop_prob\")\n",
    "\n",
    "tf.add_to_collection(\"infer_input\", q_input)\n",
    "tf.add_to_collection(\"infer_input\", q_char_input)\n",
    "tf.add_to_collection(\"infer_input\", c_input)\n",
    "tf.add_to_collection(\"infer_input\", c_char_input)\n",
    "tf.add_to_collection(\"infer_input\", dp)\n",
    "\n",
    "with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "    # input embedding layer\n",
    "    with tf.variable_scope(\"W_Embedding\"):\n",
    "        pretrained_embedding = tf.get_variable(\"w_embedding\",\n",
    "                                               shape=[72497, 50],\n",
    "                                               initializer=tf.constant_initializer(embedding[:-2,:]),\n",
    "                                               trainable=False)\n",
    "        unknown_embedding = tf.get_variable(\"unknown\",\n",
    "                                            shape=[1, 50],\n",
    "                                            initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                            trainable=True)\n",
    "        padding_embedding = tf.get_variable(\"padding\",\n",
    "                                            shape=[1, 50],\n",
    "                                            initializer=tf.zeros_initializer(),\n",
    "                                            trainable=False)\n",
    "        word_embedding = tf.concat([pretrained_embedding, unknown_embedding, padding_embedding], 0)\n",
    "        q_embed = tf.nn.embedding_lookup(word_embedding, q_input)\n",
    "        c_embed = tf.nn.embedding_lookup(word_embedding, c_input)\n",
    "\n",
    "    with tf.variable_scope(\"C_Embedding\"):\n",
    "        char_embedding = tf.get_variable(\"c_embedding\",\n",
    "                                         shape=[215, 200],\n",
    "                                         initializer=tf.random_uniform_initializer(-0.5,0.5),\n",
    "                                         trainable=True)\n",
    "        padding = tf.get_variable(\"padding\",\n",
    "                                  shape=[1, 200],\n",
    "                                  initializer=tf.zeros_initializer(),\n",
    "                                  trainable=False)\n",
    "        char_combined = tf.concat([char_embedding, padding], 0, name=\"char_embedding\")\n",
    "        q_char_embed = tf.nn.embedding_lookup(char_combined, q_char_input)\n",
    "        c_char_embed = tf.nn.embedding_lookup(char_combined, c_char_input)\n",
    "        squeeze_to_word_q = tf.layers.max_pooling1d(q_char_embed, 16, 16)\n",
    "        squeeze_to_word_c = tf.layers.max_pooling1d(c_char_embed, 16, 16)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_output\"):\n",
    "        q_embed_out = tf.concat([q_embed, squeeze_to_word_q], 2)\n",
    "        q_embed_out = tf.nn.dropout(q_embed_out, 1-dp)\n",
    "        c_embed_out = tf.concat([c_embed, squeeze_to_word_c], 2)\n",
    "        c_embed_out = tf.nn.dropout(c_embed_out, 1-dp*0.5)\n",
    "        q_embed_out = highway(\"highway\", q_embed_out)\n",
    "        c_embed_out = highway(\"highway\", c_embed_out)\n",
    "\n",
    "with tf.variable_scope(\"Embedding_Encoder_Layer\"):\n",
    "    # embedding encoder layer\n",
    "    q_encoded = embedding_encoder_block(\"encoder_block\", q_embed_out)\n",
    "    c_encoded = embedding_encoder_block(\"encoder_block\", c_embed_out)\n",
    "    print(q_encoded.shape, c_encoded.shape)\n",
    "    \n",
    "with tf.variable_scope(\"Context_Query_Attention_Layer\"):\n",
    "    # context_query attention layer\n",
    "    # first compute similarity matrix between context and query\n",
    "    # S_tj = w * [C_t; Q_j; C_t*Q_j]\n",
    "    c_expand = tf.expand_dims(c_encoded, 2)\n",
    "    c_expand = tf.tile(c_expand, [1,1,30,1])\n",
    "    q_expand = tf.expand_dims(q_encoded, 1)\n",
    "    q_expand = tf.tile(q_expand, [1,240,1,1])\n",
    "    qc_mul = tf.multiply(c_expand, q_expand)\n",
    "    concat = tf.concat([c_expand,q_expand,qc_mul], 3)\n",
    "    w = tf.get_variable(\"s_w\", [384,1])\n",
    "    \n",
    "    # similarity matrix S (logits)\n",
    "    S = tf.einsum(\"abcde,ef->abcdf\", tf.expand_dims(concat,3),w)\n",
    "    S = tf.squeeze(S,[-2,-1])\n",
    "    # S_: softmax over rows\n",
    "    S_ = tf.nn.softmax(S)\n",
    "    # S__T: transpose of softmax over coloum\n",
    "    S__T = tf.transpose(tf.nn.softmax(S, axis=1),[0,2,1])\n",
    "    # context_query attention\n",
    "    A = tf.matmul(S_, q_encoded)\n",
    "    # query_context attention\n",
    "    B = tf.matmul(tf.matmul(S_, S__T), c_encoded)\n",
    "    \n",
    "    # layer output\n",
    "    G = tf.concat([c_encoded, A, tf.multiply(c_encoded,A), tf.multiply(c_encoded,B)],2)\n",
    "    print(G.shape)\n",
    "\n",
    "with tf.variable_scope(\"Model_Encoder_Layer\"):\n",
    "    # model encoder layer\n",
    "    G_conv = tf.layers.separable_conv1d(G, 128, 1, padding=\"same\", activation=tf.nn.relu)\n",
    "    model_encoder1 = model_encoder_block(\"model_encoder\", G_conv)\n",
    "    model_encoder2 = model_encoder_block(\"model_encoder\", model_encoder1)\n",
    "    model_encoder3 = model_encoder_block(\"model_encoder\", model_encoder2)\n",
    "    print(model_encoder1.shape,model_encoder2.shape,model_encoder3.shape)\n",
    "\n",
    "global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    # output layer\n",
    "    p1_input = tf.concat([model_encoder1, model_encoder2],2)\n",
    "    p2_input = tf.concat([model_encoder2, model_encoder3],2)\n",
    "    p1_prob = tf.nn.softmax(tf.squeeze(tf.layers.conv1d(p1_input, 1, 1),-1))\n",
    "    p2_prob = tf.nn.softmax(tf.squeeze(tf.layers.conv1d(p2_input, 1, 1),-1))\n",
    "    pred_s = tf.argmax(p1_prob, axis=1)\n",
    "    pred_e = tf.argmax(p2_prob, axis=1)\n",
    "    s_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(start_mask,1)],1)\n",
    "    e_pairs = tf.concat([tf.expand_dims(tf.range(batch_size),1), tf.expand_dims(end_mask,1)],1)\n",
    "    yhat_p1 = tf.add(tf.gather_nd(p1_prob, s_pairs), 1e-15)\n",
    "    yhat_p2 = tf.add(tf.gather_nd(p2_prob, e_pairs), 1e-15)\n",
    "\n",
    "tf.add_to_collection(\"predictions\", p1_prob)\n",
    "tf.add_to_collection(\"predictions\", p2_prob)\n",
    "    \n",
    "with tf.variable_scope(\"Optimizer\"):\n",
    "    # add l2 weight decay to all variables\n",
    "    trainables = tf.trainable_variables()\n",
    "    loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in trainables if 'bias' not in v.name ]) * 3e-7\n",
    "    loss = -tf.reduce_mean(tf.log(yhat_p1) + tf.log(yhat_p2)) + loss_l2\n",
    "    \n",
    "    # perform cold warm up and gradient clipping\n",
    "    lr = tf.minimum(0.001, 0.001 / tf.log(999.) * tf.log(tf.cast(global_step, tf.float32) + 1))\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=0.8,epsilon=1e-7)\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    opt_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "    \n",
    "    # apply exponential moving average\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "    with tf.control_dependencies([opt_op]):\n",
    "        train_step = ema.apply(trainables)\n",
    "\n",
    "tf.add_to_collection(\"train_step\", train_step)\n",
    "        \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "f_measure = tf.get_variable(\"f_measure\", (), trainable=False)\n",
    "tf.summary.scalar(\"f_measure\", f_measure)\n",
    "print(yhat_p1.shape, yhat_p2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
